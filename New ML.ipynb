{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nasdaqdatalink\n",
    "\n",
    "pd.set_option(\"mode.copy_on_write\", True)\n",
    "\n",
    "SEED = 7\n",
    "np.random.seed(SEED)\n",
    "\n",
    "START = \"2005-01-01\"\n",
    "END = \"2025-12-31\"\n",
    "\n",
    "MIN_PRICE = 5.0\n",
    "MIN_DVOL_20 = 5_000_000\n",
    "\n",
    "N_LONG = 50\n",
    "N_SHORT = 50\n",
    "\n",
    "COST_BPS_ONE_WAY = 5.0\n",
    "FUNDAMENTALS_LAG_TRADING_DAYS = 1\n",
    "\n",
    "OUTPUT_XLSX = \"monthly_holdings.xlsx\"\n",
    "\n",
    "CACHE_DIR = Path(\"cache_sharadar\")\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "nasdaqdatalink.ApiConfig.api_key = \"vo3osJWr68eTVPawaV_B\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path.cwd()\n",
    "while PROJECT_ROOT.name != \"Bison\" and PROJECT_ROOT.parent != PROJECT_ROOT:\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "SEP_CACHE_DIR = (PROJECT_ROOT / \"cache_sharadar\" / \"sep_by_year\").resolve()\n",
    "SEP_CACHE_DIR = Path(SEP_CACHE_DIR)\n",
    "SEP_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _cache_max_date(cache_dir: Path):\n",
    "    mx = None\n",
    "    for p in sorted(cache_dir.glob(\"sep_*.parquet\")):\n",
    "        try:\n",
    "            df = pd.read_parquet(p, columns=[\"date\"])\n",
    "            d = pd.to_datetime(df[\"date\"], errors=\"coerce\").max()\n",
    "            if d is not pd.NaT:\n",
    "                mx = d if mx is None else max(mx, d)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return mx\n",
    "\n",
    "def _merge_into_year_cache(sep_new: pd.DataFrame, cache_dir: Path):\n",
    "    df = sep_new.copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"ticker\"] = df[\"ticker\"].astype(str)\n",
    "    df = df.dropna(subset=[\"date\", \"ticker\"])\n",
    "    df[\"year\"] = df[\"date\"].dt.year\n",
    "\n",
    "    wrote = []\n",
    "    for y, g in df.groupby(\"year\"):\n",
    "        p = cache_dir / f\"sep_{int(y)}.parquet\"\n",
    "        g = g.drop(columns=[\"year\"])\n",
    "        if p.exists():\n",
    "            old = pd.read_parquet(p)\n",
    "            old[\"date\"] = pd.to_datetime(old[\"date\"], errors=\"coerce\")\n",
    "            old[\"ticker\"] = old[\"ticker\"].astype(str)\n",
    "            out = pd.concat([old, g], ignore_index=True)\n",
    "        else:\n",
    "            out = g\n",
    "\n",
    "        out = out.drop_duplicates(subset=[\"ticker\", \"date\"], keep=\"last\")\n",
    "        out = out.sort_values([\"date\", \"ticker\"]).reset_index(drop=True)\n",
    "        out.to_parquet(p)\n",
    "        wrote.append((int(y), len(out), out[\"date\"].max().date()))\n",
    "    return wrote\n",
    "\n",
    "cache_last = _cache_max_date(SEP_CACHE_DIR)\n",
    "if cache_last is None:\n",
    "    raise ValueError(\"No existing sep_YYYY.parquet files found to infer start date.\")\n",
    "\n",
    "start = (pd.Timestamp(cache_last).normalize() + pd.Timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "end = pd.Timestamp.today().normalize().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(\"Cache last date:\", pd.Timestamp(cache_last).date())\n",
    "print(\"Downloading SEP from:\", start, \"to\", end)\n",
    "\n",
    "api_key = \"vo3osJWr68eTVPawaV_B\"\n",
    "\n",
    "import nasdaqdatalink as ndl\n",
    "ndl.ApiConfig.api_key = api_key\n",
    "\n",
    "def _download_sep_window(s, e):\n",
    "    return ndl.get_table(\n",
    "        \"SHARADAR/SEP\",\n",
    "        date={\"gte\": s, \"lte\": e},\n",
    "        paginate=True,\n",
    "    )\n",
    "\n",
    "windows = []\n",
    "s = pd.Timestamp(start)\n",
    "e = pd.Timestamp(end)\n",
    "cur = s\n",
    "while cur <= e:\n",
    "    nxt = (cur + pd.offsets.MonthEnd(1)).normalize()\n",
    "    w_end = min(nxt, e)\n",
    "    windows.append((cur.strftime(\"%Y-%m-%d\"), w_end.strftime(\"%Y-%m-%d\")))\n",
    "    cur = (w_end + pd.Timedelta(days=1)).normalize()\n",
    "\n",
    "dfs = []\n",
    "for s1, e1 in windows:\n",
    "    df = _download_sep_window(s1, e1)\n",
    "    if df is not None and len(df):\n",
    "        dfs.append(df)\n",
    "\n",
    "if not dfs:\n",
    "    print(\"No new rows returned. Cache is already up to date for this window.\")\n",
    "else:\n",
    "    sep_new = pd.concat(dfs, ignore_index=True)\n",
    "    wrote = _merge_into_year_cache(sep_new, SEP_CACHE_DIR)\n",
    "    print(\"Updated year files:\")\n",
    "    for y, n, mx in wrote:\n",
    "        print(f\"sep_{y}.parquet | rows={n} | max_date={mx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def _chunk(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]\n",
    "\n",
    "def _next_trading_day(trading_days_index: pd.DatetimeIndex, d: pd.Timestamp):\n",
    "    d = pd.Timestamp(d)\n",
    "    pos = trading_days_index.searchsorted(d)\n",
    "    if pos >= len(trading_days_index) - 1:\n",
    "        return None\n",
    "    if trading_days_index[pos] == d:\n",
    "        return trading_days_index[pos + 1]\n",
    "    return trading_days_index[pos]\n",
    "\n",
    "def _shift_trading_days(trading_days_index, d, k):\n",
    "    d = pd.Timestamp(d)\n",
    "    pos = trading_days_index.searchsorted(d)\n",
    "    if pos >= len(trading_days_index):\n",
    "        return None\n",
    "    if trading_days_index[pos] != d:\n",
    "        pos -= 1\n",
    "    pos2 = pos + int(k)\n",
    "    if pos2 < 0 or pos2 >= len(trading_days_index):\n",
    "        return None\n",
    "    return trading_days_index[pos2]\n",
    "\n",
    "def _quarter_end_trading_days(trading_days: pd.DatetimeIndex):\n",
    "    trading_days = pd.DatetimeIndex(pd.to_datetime(trading_days)).sort_values().unique()\n",
    "    month_ends = (\n",
    "        pd.Series(trading_days)\n",
    "        .groupby([trading_days.year, trading_days.month])\n",
    "        .max()\n",
    "        .values\n",
    "    )\n",
    "    month_ends = pd.DatetimeIndex(month_ends).sort_values().unique()\n",
    "    q_ends = month_ends[month_ends.is_quarter_end]\n",
    "    return pd.DatetimeIndex(q_ends).sort_values().unique()\n",
    "\n",
    "def _save_parquet(df, path):\n",
    "    df.to_parquet(path, index=False)\n",
    "\n",
    "def _load_parquet(path):\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def fetch_tickers_universe(table=\"SEP\"):\n",
    "    path = Path(CACHE_DIR) / f\"tickers_{table}.parquet\"\n",
    "    if path.exists():\n",
    "        return _load_parquet(path)\n",
    "\n",
    "    df = nasdaqdatalink.get_table(\"SHARADAR/TICKERS\", table=table, paginate=True)\n",
    "    if df is None or df.empty:\n",
    "        raise ValueError(\"SHARADAR/TICKERS returned no rows.\")\n",
    "    if \"ticker\" not in df.columns:\n",
    "        raise ValueError(\"SHARADAR/TICKERS did not return 'ticker'.\")\n",
    "\n",
    "    if \"category\" in df.columns:\n",
    "        df = df[\n",
    "            df[\"category\"].isin(\n",
    "                [\n",
    "                    \"Domestic Common Stock\",\n",
    "                    \"Domestic Common Stock Primary\",\n",
    "                    \"Domestic Common Stock Secondary\",\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    df = df.dropna(subset=[\"ticker\"]).reset_index(drop=True)\n",
    "    _save_parquet(df, path)\n",
    "    return df\n",
    "\n",
    "def build_trade_universe(univ, start, end):\n",
    "    u = univ.copy()\n",
    "    start_dt = pd.Timestamp(start)\n",
    "    end_dt = pd.Timestamp(end)\n",
    "\n",
    "    if \"firstpricedate\" in u.columns:\n",
    "        u[\"firstpricedate\"] = pd.to_datetime(u[\"firstpricedate\"], errors=\"coerce\")\n",
    "    if \"lastpricedate\" in u.columns:\n",
    "        u[\"lastpricedate\"] = pd.to_datetime(u[\"lastpricedate\"], errors=\"coerce\")\n",
    "\n",
    "    if \"firstpricedate\" in u.columns and \"lastpricedate\" in u.columns:\n",
    "        u = u[(u[\"firstpricedate\"] <= end_dt) & (u[\"lastpricedate\"] >= start_dt)]\n",
    "\n",
    "    return u[\"ticker\"].dropna().unique().tolist()\n",
    "\n",
    "def fetch_sep_prices_yearly(tickers, start, end, columns=None, chunk_size=150, cache_name=\"sep_full\"):\n",
    "    start_dt = pd.Timestamp(start)\n",
    "    end_dt = pd.Timestamp(end)\n",
    "\n",
    "    if columns is None:\n",
    "        columns = [\"ticker\", \"date\", \"closeadj\", \"close\", \"volume\"]\n",
    "\n",
    "    path = Path(CACHE_DIR) / f\"{cache_name}_{start_dt.date()}_{end_dt.date()}_c{chunk_size}.parquet\"\n",
    "    if path.exists():\n",
    "        return _load_parquet(path)\n",
    "\n",
    "    years = range(start_dt.year, end_dt.year + 1)\n",
    "    out = []\n",
    "\n",
    "    for y in years:\n",
    "        w0 = max(pd.Timestamp(f\"{y}-01-01\"), start_dt)\n",
    "        w1 = min(pd.Timestamp(f\"{y}-12-31\"), end_dt)\n",
    "        if w0 > w1:\n",
    "            continue\n",
    "\n",
    "        w0s = str(w0.date())\n",
    "        w1s = str(w1.date())\n",
    "\n",
    "        for tk_chunk in _chunk(list(tickers), chunk_size):\n",
    "            df = nasdaqdatalink.get_table(\n",
    "                \"SHARADAR/SEP\",\n",
    "                ticker=tk_chunk,\n",
    "                date={\"gte\": w0s, \"lte\": w1s},\n",
    "                qopts={\"columns\": columns},\n",
    "                paginate=True,\n",
    "            )\n",
    "            if df is not None and not df.empty:\n",
    "                out.append(df)\n",
    "\n",
    "    px = pd.concat(out, ignore_index=True) if out else pd.DataFrame()\n",
    "    if px.empty:\n",
    "        raise ValueError(\"SEP pull returned no rows. Check tickers/date range.\")\n",
    "\n",
    "    px[\"date\"] = pd.to_datetime(px[\"date\"])\n",
    "    _save_parquet(px, path)\n",
    "    return px\n",
    "\n",
    "def fetch_sfp_prices_yearly(tickers, start, end, columns=None, chunk_size=200, cache_name=\"sfp_full\"):\n",
    "    start_dt = pd.Timestamp(start)\n",
    "    end_dt = pd.Timestamp(end)\n",
    "\n",
    "    if columns is None:\n",
    "        columns = [\"ticker\", \"date\", \"closeadj\", \"close\", \"volume\"]\n",
    "\n",
    "    path = Path(CACHE_DIR) / f\"{cache_name}_{start_dt.date()}_{end_dt.date()}_c{chunk_size}.parquet\"\n",
    "    if path.exists():\n",
    "        return _load_parquet(path)\n",
    "\n",
    "    years = range(start_dt.year, end_dt.year + 1)\n",
    "    out = []\n",
    "\n",
    "    for y in years:\n",
    "        w0 = max(pd.Timestamp(f\"{y}-01-01\"), start_dt)\n",
    "        w1 = min(pd.Timestamp(f\"{y}-12-31\"), end_dt)\n",
    "        if w0 > w1:\n",
    "            continue\n",
    "\n",
    "        w0s = str(w0.date())\n",
    "        w1s = str(w1.date())\n",
    "\n",
    "        for tk_chunk in _chunk(list(tickers), chunk_size):\n",
    "            df = nasdaqdatalink.get_table(\n",
    "                \"SHARADAR/SFP\",\n",
    "                ticker=tk_chunk,\n",
    "                date={\"gte\": w0s, \"lte\": w1s},\n",
    "                qopts={\"columns\": columns},\n",
    "                paginate=True,\n",
    "            )\n",
    "            if df is not None and not df.empty:\n",
    "                out.append(df)\n",
    "\n",
    "    px = pd.concat(out, ignore_index=True) if out else pd.DataFrame()\n",
    "    if px.empty:\n",
    "        raise ValueError(\"SFP pull returned no rows. Check entitlement/tickers/date range.\")\n",
    "\n",
    "    px[\"date\"] = pd.to_datetime(px[\"date\"])\n",
    "    _save_parquet(px, path)\n",
    "    return px\n",
    "\n",
    "def fetch_sf1_fundamentals_all_numeric(tickers, start, end, dimension=\"ART\", chunk_size=200, cache_name=\"sf1_allnum\"):\n",
    "    start_s = str(pd.Timestamp(start).date())\n",
    "    end_s = str(pd.Timestamp(end).date())\n",
    "\n",
    "    path = Path(CACHE_DIR) / f\"{cache_name}_{dimension}_{start_s}_{end_s}_c{chunk_size}.parquet\"\n",
    "    if path.exists():\n",
    "        return _load_parquet(path)\n",
    "\n",
    "    out = []\n",
    "    for tk_chunk in _chunk(list(tickers), chunk_size):\n",
    "        df = nasdaqdatalink.get_table(\n",
    "            \"SHARADAR/SF1\",\n",
    "            ticker=tk_chunk,\n",
    "            dimension=dimension,\n",
    "            datekey={\"gte\": start_s, \"lte\": end_s},\n",
    "            paginate=True,\n",
    "        )\n",
    "        if df is not None and not df.empty:\n",
    "            out.append(df)\n",
    "\n",
    "    f = pd.concat(out, ignore_index=True) if out else pd.DataFrame()\n",
    "    if f.empty:\n",
    "        return f\n",
    "\n",
    "    f[\"datekey\"] = pd.to_datetime(f[\"datekey\"], errors=\"coerce\")\n",
    "    if \"calendardate\" in f.columns:\n",
    "        f[\"calendardate\"] = pd.to_datetime(f[\"calendardate\"], errors=\"coerce\")\n",
    "\n",
    "    keep_id = {\"ticker\", \"dimension\", \"datekey\", \"calendardate\"}\n",
    "    num_cols = [c for c in f.columns if c in keep_id or pd.api.types.is_numeric_dtype(f[c])]\n",
    "    f = f[num_cols]\n",
    "\n",
    "    _save_parquet(f, path)\n",
    "    return f\n",
    "\n",
    "def pick_benchmark(as_of_date):\n",
    "    d = str(pd.Timestamp(as_of_date).date())\n",
    "\n",
    "    for tk in [\"SPX\", \"^GSPC\"]:\n",
    "        try:\n",
    "            df = nasdaqdatalink.get_table(\n",
    "                \"SHARADAR/SEP\",\n",
    "                ticker=tk,\n",
    "                date={\"eq\": d},\n",
    "                qopts={\"columns\": [\"ticker\", \"date\", \"close\", \"closeadj\"]},\n",
    "                paginate=False,\n",
    "            )\n",
    "            if df is not None and not df.empty:\n",
    "                return (\"SEP\", tk)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    for tk in [\"SPY\", \"IVV\", \"VOO\"]:\n",
    "        try:\n",
    "            df = nasdaqdatalink.get_table(\n",
    "                \"SHARADAR/SFP\",\n",
    "                ticker=tk,\n",
    "                date={\"eq\": d},\n",
    "                qopts={\"columns\": [\"ticker\", \"date\", \"close\", \"closeadj\"]},\n",
    "                paginate=False,\n",
    "            )\n",
    "            if df is not None and not df.empty:\n",
    "                return (\"SFP\", tk)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return (\"SFP\", \"SPY\")\n",
    "\n",
    "def _load_year(y):\n",
    "    p = SEP_CACHE_DIR / f\"sep_{y}.parquet\"\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    df = pd.read_parquet(p)\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"ticker\", \"date\"])\n",
    "    df[\"ticker\"] = df[\"ticker\"].astype(str)\n",
    "    return df\n",
    "\n",
    "def load_sep_from_year_cache(start, end):\n",
    "    start = pd.Timestamp(start)\n",
    "    end = pd.Timestamp(end)\n",
    "    years = range(start.year, end.year + 1)\n",
    "\n",
    "    dfs = []\n",
    "    missing = []\n",
    "    for y in years:\n",
    "        df = _load_year(y)\n",
    "        if df is None:\n",
    "            missing.append(y)\n",
    "            continue\n",
    "        df = df[(df[\"date\"] >= start) & (df[\"date\"] <= end)]\n",
    "        if not df.empty:\n",
    "            dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        raise ValueError(f\"No SEP data loaded. Missing years: {missing}. Dir: {SEP_CACHE_DIR}\")\n",
    "\n",
    "    px = pd.concat(dfs, ignore_index=True)\n",
    "    px = px.drop_duplicates(subset=[\"ticker\", \"date\"], keep=\"last\")\n",
    "    px = px.sort_values([\"date\", \"ticker\"]).reset_index(drop=True)\n",
    "    return px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univ = fetch_tickers_universe(table=\"SEP\")\n",
    "tickers = build_trade_universe(univ, START, END)\n",
    "\n",
    "BENCH_SOURCE, BENCH_TICKER = pick_benchmark(END)\n",
    "BENCH_SOURCE, BENCH_TICKER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_stocks = load_sep_from_year_cache(START, END)\n",
    "print(\n",
    "    \"px_stocks:\",\n",
    "    px_stocks.shape,\n",
    "    px_stocks[\"date\"].min(),\n",
    "    px_stocks[\"date\"].max(),\n",
    "    px_stocks[\"ticker\"].nunique(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"px_stocks\" not in globals() or px_stocks is None or len(px_stocks) == 0:\n",
    "    raise NameError(\"px_stocks not found. Load SEP first (long table with date/ticker/price).\")\n",
    "\n",
    "px = px_stocks.copy()\n",
    "px[\"date\"] = pd.to_datetime(px[\"date\"], errors=\"coerce\")\n",
    "px[\"ticker\"] = px[\"ticker\"].astype(str)\n",
    "px = px.dropna(subset=[\"date\", \"ticker\"])\n",
    "\n",
    "def _pick_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "close_col = _pick_col(px, [\"close\", \"Close\", \"adj_close\", \"Adj Close\", \"closeadj\", \"price\", \"last\"])\n",
    "if close_col is None:\n",
    "    raise ValueError(f\"Could not find a close column in px_stocks. Columns: {list(px.columns)[:50]}\")\n",
    "\n",
    "dvol_col = _pick_col(px, [\"dollar_volume\", \"dvol\", \"dollarvol\", \"dv\", \"dvol_20\", \"dollarvol20\"])\n",
    "vol_col = _pick_col(px, [\"volume\", \"vol\", \"shares\", \"share_volume\"])\n",
    "\n",
    "px[close_col] = pd.to_numeric(px[close_col], errors=\"coerce\")\n",
    "\n",
    "stock_close = (\n",
    "    px.pivot_table(index=\"date\", columns=\"ticker\", values=close_col, aggfunc=\"last\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "stock_dvol = None\n",
    "if dvol_col is not None:\n",
    "    px[dvol_col] = pd.to_numeric(px[dvol_col], errors=\"coerce\")\n",
    "    stock_dvol = (\n",
    "        px.pivot_table(index=\"date\", columns=\"ticker\", values=dvol_col, aggfunc=\"last\")\n",
    "        .sort_index()\n",
    "    )\n",
    "elif vol_col is not None:\n",
    "    px[vol_col] = pd.to_numeric(px[vol_col], errors=\"coerce\")\n",
    "    stock_dvol = (\n",
    "        px.pivot_table(index=\"date\", columns=\"ticker\", values=vol_col, aggfunc=\"last\")\n",
    "        .sort_index()\n",
    "    )\n",
    "    stock_dvol = stock_dvol * stock_close\n",
    "\n",
    "stock_vol = stock_dvol\n",
    "\n",
    "print(\n",
    "    \"stock_close:\",\n",
    "    stock_close.shape,\n",
    "    \"| date range:\",\n",
    "    stock_close.index.min().date(),\n",
    "    \"->\",\n",
    "    stock_close.index.max().date(),\n",
    ")\n",
    "if stock_dvol is not None:\n",
    "    print(\n",
    "        \"stock_dvol:\",\n",
    "        stock_dvol.shape,\n",
    "        \"| date range:\",\n",
    "        stock_dvol.index.min().date(),\n",
    "        \"->\",\n",
    "        stock_dvol.index.max().date(),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_all_days():\n",
    "    if \"stock_close\" in globals() and isinstance(stock_close, pd.DataFrame) and not stock_close.empty:\n",
    "        return pd.DatetimeIndex(pd.to_datetime(stock_close.index)).sort_values().unique()\n",
    "\n",
    "    for nm in [\"stock_px\", \"px\", \"prices\", \"daily_prices\"]:\n",
    "        obj = globals().get(nm, None)\n",
    "        if isinstance(obj, pd.DataFrame) and not obj.empty and not isinstance(obj.index, pd.MultiIndex):\n",
    "            return pd.DatetimeIndex(pd.to_datetime(obj.index)).sort_values().unique()\n",
    "\n",
    "    if \"px_stocks\" in globals() and isinstance(px_stocks, pd.DataFrame) and not px_stocks.empty:\n",
    "        if \"date\" not in px_stocks.columns:\n",
    "            raise ValueError(\"px_stocks exists but has no 'date' column.\")\n",
    "        return pd.DatetimeIndex(pd.to_datetime(px_stocks[\"date\"], errors=\"coerce\").dropna().unique()).sort_values()\n",
    "\n",
    "    if \"X_price\" in globals() and isinstance(X_price, pd.DataFrame) and not X_price.empty:\n",
    "        if isinstance(X_price.index, pd.MultiIndex):\n",
    "            return pd.DatetimeIndex(pd.to_datetime(X_price.index.get_level_values(\"date\"))).sort_values().unique()\n",
    "        return pd.DatetimeIndex(pd.to_datetime(X_price.index)).sort_values().unique()\n",
    "\n",
    "    raise NameError(\"No daily (or at least dated) object found. Need stock_close, px_stocks, or X_price to build trading days.\")\n",
    "\n",
    "all_days = _get_all_days()\n",
    "q_ends = _quarter_end_trading_days(all_days)\n",
    "\n",
    "pairs = []\n",
    "for i in range(len(q_ends) - 1):\n",
    "    sig = pd.Timestamp(q_ends[i])\n",
    "    entry = _next_trading_day(all_days, sig)\n",
    "    exit_ = pd.Timestamp(q_ends[i + 1])\n",
    "    if entry is None:\n",
    "        continue\n",
    "    if entry > exit_:\n",
    "        continue\n",
    "    pairs.append((sig, entry, exit_))\n",
    "\n",
    "if not pairs:\n",
    "    raise ValueError(\"No valid quarterly entry/exit pairs found. Check date coverage.\")\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs, columns=[\"signal_date\", \"trade_date\", \"exit_date\"]).set_index(\"signal_date\")\n",
    "signal_dates = pairs_df.index\n",
    "\n",
    "print(\"Trading days:\", len(all_days), \"| range:\", all_days.min().date(), \"->\", all_days.max().date())\n",
    "print(\"Quarter pairs:\", len(pairs_df))\n",
    "print(\"Signal range:\", pairs_df.index.min().date(), \"->\", pairs_df.index.max().date())\n",
    "pairs_df.head(), pairs_df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_df = pairs_df.sort_index()\n",
    "\n",
    "trade_px = stock_close.reindex(pairs_df[\"trade_date\"].values)\n",
    "exit_px = stock_close.reindex(pairs_df[\"exit_date\"].values)\n",
    "\n",
    "trade_px.index = pairs_df.index\n",
    "exit_px.index = pairs_df.index\n",
    "\n",
    "fwd_stock = (exit_px / trade_px - 1.0).astype(\"float32\")\n",
    "fwd_stock.index.name = \"signal_date\"\n",
    "\n",
    "sig_px = stock_close.reindex(pairs_df.index)\n",
    "\n",
    "eligible = sig_px >= MIN_PRICE\n",
    "\n",
    "if stock_vol is not None:\n",
    "    dvol_20_daily = (stock_vol * stock_close).rolling(20, min_periods=20).mean()\n",
    "    dvol_20_sig = dvol_20_daily.reindex(pairs_df.index)\n",
    "    eligible = eligible & (dvol_20_sig >= MIN_DVOL_20)\n",
    "\n",
    "eligible = eligible & trade_px.notna() & exit_px.notna()\n",
    "\n",
    "elig_counts = eligible.sum(axis=1).astype(int)\n",
    "print(\"Eligible names per month:\")\n",
    "print(elig_counts.describe())\n",
    "\n",
    "low_cut = int(elig_counts.quantile(0.05))\n",
    "print(\"\n",
    "Months below 5th pct threshold (\", low_cut, \"):\")\n",
    "print(elig_counts[elig_counts <= low_cut].head(30))\n",
    "\n",
    "fwd_bench = fwd_stock.where(eligible).mean(axis=1).astype(\"float32\")\n",
    "fwd_bench.name = \"bench_fwd\"\n",
    "\n",
    "y_excess = fwd_stock.sub(fwd_bench, axis=0).stack(future_stack=True).astype(\"float32\")\n",
    "y_excess.index.set_names([\"date\", \"ticker\"], inplace=True)\n",
    "y_excess.name = \"y\"\n",
    "\n",
    "print(\"\n",
    "Shapes:\")\n",
    "print(\"pairs_df:\", pairs_df.shape)\n",
    "print(\"fwd_stock:\", fwd_stock.shape)\n",
    "print(\"fwd_bench:\", fwd_bench.shape)\n",
    "print(\"y_excess:\", y_excess.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = Path(CACHE_DIR) / f\"X_price_{pd.Timestamp(START).date()}_{pd.Timestamp(END).date()}.parquet\"\n",
    "\n",
    "if cache_path.exists():\n",
    "    X_price = pd.read_parquet(cache_path)\n",
    "    X_price.index = (\n",
    "        pd.MultiIndex.from_frame(X_price.index.to_frame(index=False).rename(columns={0: \"date\", 1: \"ticker\"}))\n",
    "        if not isinstance(X_price.index, pd.MultiIndex)\n",
    "        else X_price.index\n",
    "    )\n",
    "    X_price.index.set_names([\"date\", \"ticker\"], inplace=True)\n",
    "    print(\"Loaded cached X_price:\", X_price.shape)\n",
    "else:\n",
    "    stock_close = stock_close.sort_index()\n",
    "    if stock_vol is not None:\n",
    "        stock_vol = stock_vol.reindex(stock_close.index).sort_index()\n",
    "\n",
    "    sig = pd.Index(pd.to_datetime(signal_dates)).unique().sort_values()\n",
    "    sig = sig.intersection(stock_close.index)\n",
    "\n",
    "    print(\"signal months:\", len(sig))\n",
    "    print(\"tickers:\", stock_close.shape[1])\n",
    "    print(\"price range:\", stock_close.index.min(), \"->\", stock_close.index.max())\n",
    "\n",
    "    daily_ret = stock_close.pct_change(fill_method=None)\n",
    "\n",
    "    ret_21 = stock_close / stock_close.shift(21) - 1.0\n",
    "    ret_63 = stock_close / stock_close.shift(63) - 1.0\n",
    "    ret_126 = stock_close / stock_close.shift(126) - 1.0\n",
    "    ret_252 = stock_close / stock_close.shift(252) - 1.0\n",
    "\n",
    "    vol_63 = daily_ret.rolling(63).std()\n",
    "    vol_252 = daily_ret.rolling(252).std()\n",
    "\n",
    "    dvol_20 = None\n",
    "    if stock_vol is not None:\n",
    "        dvol_20 = (stock_vol * stock_close).rolling(20).mean()\n",
    "\n",
    "    def _stack_at(mat, name):\n",
    "        tmp = mat.loc[sig].stack(future_stack=True).rename(name).to_frame()\n",
    "        tmp.index.set_names([\"date\", \"ticker\"], inplace=True)\n",
    "        return tmp\n",
    "\n",
    "    X_parts = [\n",
    "        _stack_at(ret_21, \"ret_21\"),\n",
    "        _stack_at(ret_63, \"ret_63\"),\n",
    "        _stack_at(ret_126, \"ret_126\"),\n",
    "        _stack_at(ret_252, \"ret_252\"),\n",
    "        _stack_at(vol_63, \"vol_63\"),\n",
    "        _stack_at(vol_252, \"vol_252\"),\n",
    "    ]\n",
    "\n",
    "    if dvol_20 is not None:\n",
    "        X_parts.append(_stack_at(dvol_20, \"dvol_20\"))\n",
    "\n",
    "    px_sig = stock_close.loc[sig].stack(future_stack=True).rename(\"px\").to_frame()\n",
    "    px_sig.index.set_names([\"date\", \"ticker\"], inplace=True)\n",
    "    X_parts.append(px_sig)\n",
    "\n",
    "    X_price = pd.concat(X_parts, axis=1).sort_index()\n",
    "    X_price = X_price[~X_price.index.duplicated(keep=\"last\")]\n",
    "\n",
    "    for c in X_price.columns:\n",
    "        X_price[c] = X_price[c].astype(\"float32\", copy=False)\n",
    "\n",
    "    X_price.to_parquet(cache_path)\n",
    "    print(\"Saved X_price:\", cache_path)\n",
    "\n",
    "print(\"X_price:\", X_price.shape)\n",
    "print(\"NA rate (top 12):\")\n",
    "print(X_price.isna().mean().sort_values(ascending=False).head(12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "CACHE_DIR = Path(CACHE_DIR)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DIM = \"ARQ\"\n",
    "SF1_RAW_PATH = CACHE_DIR / f\"sf1_raw_{DIM}_{pd.Timestamp(START).date()}_{pd.Timestamp(END).date()}.parquet\"\n",
    "SF1_ALIGNED_PATH = CACHE_DIR / f\"sf1_factors_aligned_{DIM}_{pd.Timestamp(START).date()}_{pd.Timestamp(END).date()}.parquet\"\n",
    "\n",
    "INDICATORS_CSV_PATH = Path(\"SF1 Indicators.csv\")\n",
    "\n",
    "ind_meta = pd.read_csv(INDICATORS_CSV_PATH)\n",
    "sf1_indicators = set(ind_meta.loc[ind_meta[\"table\"].eq(\"SF1\"), \"indicator\"].astype(str))\n",
    "\n",
    "def _to_num(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def _safe_div(a, b):\n",
    "    a = _to_num(a)\n",
    "    b = _to_num(b)\n",
    "    return a / b.replace(0, np.nan)\n",
    "\n",
    "def _safe_log_pos(x):\n",
    "    x = _to_num(x)\n",
    "    x = x.where(x > 0)\n",
    "    return np.log(x)\n",
    "\n",
    "def _neglog_pos(x):\n",
    "    return -_safe_log_pos(x)\n",
    "\n",
    "def _pct_change_4q(s):\n",
    "    s = _to_num(s)\n",
    "    return s.groupby(level=0).pct_change(4, fill_method=None)\n",
    "\n",
    "def _diff_4q(s):\n",
    "    s = _to_num(s)\n",
    "    return s.groupby(level=0).diff(4)\n",
    "\n",
    "def _shift_avail(d):\n",
    "    return _shift_trading_days(all_days, d, FUNDAMENTALS_LAG_TRADING_DAYS)\n",
    "\n",
    "def _col(name):\n",
    "    return name if name in sf1_indicators else None\n",
    "\n",
    "def _nonempty(s):\n",
    "    s = pd.Series(s)\n",
    "    return s.notna().any()\n",
    "\n",
    "if SF1_RAW_PATH.exists():\n",
    "    sf1_raw = pd.read_parquet(SF1_RAW_PATH)\n",
    "else:\n",
    "    sf1_start = (pd.Timestamp(START) - pd.Timedelta(days=365 * 2)).strftime(\"%Y-%m-%d\")\n",
    "    tickers_used = sorted(stock_close.columns.astype(str).tolist())\n",
    "    sf1_raw = fetch_sf1_fundamentals_all_numeric(\n",
    "        tickers=tickers_used,\n",
    "        start=sf1_start,\n",
    "        end=END,\n",
    "        dimension=DIM,\n",
    "        cache_name=f\"sf1_raw_{DIM}_{pd.Timestamp(START).date()}_{pd.Timestamp(END).date()}\",\n",
    "    )\n",
    "    sf1_raw.to_parquet(SF1_RAW_PATH)\n",
    "\n",
    "sf1 = sf1_raw.copy()\n",
    "sf1[\"ticker\"] = sf1[\"ticker\"].astype(str)\n",
    "sf1[\"calendardate\"] = pd.to_datetime(sf1.get(\"calendardate\", pd.NaT), errors=\"coerce\")\n",
    "sf1[\"datekey\"] = pd.to_datetime(sf1.get(\"datekey\", pd.NaT), errors=\"coerce\")\n",
    "sf1 = sf1.dropna(subset=[\"ticker\", \"calendardate\", \"datekey\"]).sort_values([\"ticker\", \"calendardate\"], kind=\"mergesort\")\n",
    "\n",
    "assets_c = _col(\"assets\")\n",
    "assetsavg_c = _col(\"assetsavg\")\n",
    "assetsc_c = _col(\"assetsc\")\n",
    "liabilitiesc_c = _col(\"liabilitiesc\")\n",
    "liabilities_c = _col(\"liabilities\")\n",
    "debt_c = _col(\"debt\")\n",
    "revenue_c = _col(\"revenue\")\n",
    "revenueusd_c = _col(\"revenueusd\")\n",
    "netinc_c = _col(\"netinc\")\n",
    "ncfo_c = _col(\"ncfo\")\n",
    "capex_c = _col(\"capex\")\n",
    "inventory_c = _col(\"inventory\")\n",
    "mcap_c = _col(\"marketcap\")\n",
    "epsdil_c = _col(\"epsdil\")\n",
    "eps_c = _col(\"eps\")\n",
    "epsusd_c = _col(\"epsusd\")\n",
    "divyield_c = _col(\"divyield\")\n",
    "pe_c = _col(\"pe\")\n",
    "pb_c = _col(\"pb\")\n",
    "ps_c = _col(\"ps\")\n",
    "evebitda_c = _col(\"evebitda\")\n",
    "grossmargin_c = _col(\"grossmargin\")\n",
    "ebitdamargin_c = _col(\"ebitdamargin\")\n",
    "netmargin_c = _col(\"netmargin\")\n",
    "currentratio_c = _col(\"currentratio\")\n",
    "assetturnover_c = _col(\"assetturnover\")\n",
    "roa_c = _col(\"roa\")\n",
    "roe_c = _col(\"roe\")\n",
    "\n",
    "sf1_keyed = sf1.set_index([\"ticker\", \"calendardate\"], drop=False)\n",
    "\n",
    "assets_base = None\n",
    "if assets_c and assets_c in sf1_keyed.columns:\n",
    "    assets_base = sf1_keyed[assets_c]\n",
    "elif assetsavg_c and assetsavg_c in sf1_keyed.columns:\n",
    "    assets_base = sf1_keyed[assetsavg_c]\n",
    "\n",
    "revenue_base = None\n",
    "if revenue_c and revenue_c in sf1_keyed.columns:\n",
    "    revenue_base = sf1_keyed[revenue_c]\n",
    "elif revenueusd_c and revenueusd_c in sf1_keyed.columns:\n",
    "    revenue_base = sf1_keyed[revenueusd_c]\n",
    "\n",
    "eps_base = None\n",
    "for nm in [epsdil_c, eps_c, epsusd_c]:\n",
    "    if nm and nm in sf1_keyed.columns:\n",
    "        eps_base = sf1_keyed[nm]\n",
    "        break\n",
    "\n",
    "val_pe = _neglog_pos(sf1_keyed[pe_c]) if (pe_c and pe_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "val_pb = _neglog_pos(sf1_keyed[pb_c]) if (pb_c and pb_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "val_ps = _neglog_pos(sf1_keyed[ps_c]) if (ps_c and ps_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "val_evebitda = _neglog_pos(sf1_keyed[evebitda_c]) if (evebitda_c and evebitda_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "\n",
    "log_mcap = _safe_log_pos(sf1_keyed[mcap_c]) if (mcap_c and mcap_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "\n",
    "grossmargin = _to_num(sf1_keyed[grossmargin_c]) if (grossmargin_c and grossmargin_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "ebitdamargin = _to_num(sf1_keyed[ebitdamargin_c]) if (ebitdamargin_c and ebitdamargin_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "netmargin = _to_num(sf1_keyed[netmargin_c]) if (netmargin_c and netmargin_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "\n",
    "roa = _to_num(sf1_keyed[roa_c]) if (roa_c and roa_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "roe = _to_num(sf1_keyed[roe_c]) if (roe_c and roe_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "assetturnover = _to_num(sf1_keyed[assetturnover_c]) if (assetturnover_c and assetturnover_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "currentratio = _to_num(sf1_keyed[currentratio_c]) if (currentratio_c and currentratio_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "\n",
    "rev_yoy = _pct_change_4q(revenue_base) if revenue_base is not None else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "eps_yoy = _pct_change_4q(eps_base) if eps_base is not None else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "asset_growth_yoy = _pct_change_4q(assets_base) if assets_base is not None else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "\n",
    "if capex_c and capex_c in sf1_keyed.columns and assets_base is not None:\n",
    "    capex_assets = _safe_div(sf1_keyed[capex_c], assets_base)\n",
    "else:\n",
    "    capex_assets = pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "\n",
    "inv_growth_yoy = _pct_change_4q(sf1_keyed[inventory_c]) if (inventory_c and inventory_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "debt_growth_yoy = _pct_change_4q(sf1_keyed[debt_c]) if (debt_c and debt_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "lev_debt_assets = _safe_div(sf1_keyed[debt_c], assets_base) if (debt_c and debt_c in sf1_keyed.columns and assets_base is not None) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "\n",
    "accruals_ni_ncfo_assets = (\n",
    "    _safe_div((_to_num(sf1_keyed[netinc_c]) - _to_num(sf1_keyed[ncfo_c])), assets_base)\n",
    "    if (netinc_c and netinc_c in sf1_keyed.columns and ncfo_c and ncfo_c in sf1_keyed.columns and assets_base is not None)\n",
    "    else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    ")\n",
    "\n",
    "if assetsc_c and assetsc_c in sf1_keyed.columns and liabilitiesc_c and liabilitiesc_c in sf1_keyed.columns and assets_base is not None:\n",
    "    wc = _to_num(sf1_keyed[assetsc_c]) - _to_num(sf1_keyed[liabilitiesc_c])\n",
    "    wc_chg_assets = _safe_div(_diff_4q(wc), assets_base)\n",
    "else:\n",
    "    wc_chg_assets = pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "\n",
    "divyield = _to_num(sf1_keyed[divyield_c]) if (divyield_c and divyield_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "\n",
    "if (not _nonempty(roa)) and (netinc_c and netinc_c in sf1_keyed.columns) and (assets_base is not None):\n",
    "    roa = _safe_div(sf1_keyed[netinc_c], assets_base)\n",
    "\n",
    "if (not _nonempty(assetturnover)) and (revenue_base is not None) and (assets_base is not None):\n",
    "    assetturnover = _safe_div(revenue_base, assets_base)\n",
    "\n",
    "sf1_f = pd.DataFrame(\n",
    "    {\n",
    "        \"ticker\": sf1_keyed[\"ticker\"].values,\n",
    "        \"datekey\": sf1_keyed[\"datekey\"].values,\n",
    "        \"calendardate\": sf1_keyed[\"calendardate\"].values,\n",
    "        \"val_pe\": val_pe.values,\n",
    "        \"val_pb\": val_pb.values,\n",
    "        \"val_ps\": val_ps.values,\n",
    "        \"val_evebitda\": val_evebitda.values,\n",
    "        \"log_mcap\": log_mcap.values,\n",
    "        \"grossmargin\": grossmargin.values,\n",
    "        \"ebitdamargin\": ebitdamargin.values,\n",
    "        \"netmargin\": netmargin.values,\n",
    "        \"roa\": roa.values,\n",
    "        \"roe\": roe.values,\n",
    "        \"currentratio\": currentratio.values,\n",
    "        \"assetturnover\": assetturnover.values,\n",
    "        \"rev_yoy\": rev_yoy.values,\n",
    "        \"eps_yoy\": eps_yoy.values,\n",
    "        \"asset_growth_yoy\": asset_growth_yoy.values,\n",
    "        \"capex_assets\": capex_assets.values,\n",
    "        \"inv_growth_yoy\": inv_growth_yoy.values,\n",
    "        \"debt_growth_yoy\": debt_growth_yoy.values,\n",
    "        \"lev_debt_assets\": lev_debt_assets.values,\n",
    "        \"accruals_ni_ncfo_assets\": accruals_ni_ncfo_assets.values,\n",
    "        \"wc_chg_assets\": wc_chg_assets.values,\n",
    "        \"divyield\": divyield.values,\n",
    "    }\n",
    ")\n",
    "\n",
    "factor_cols_all = [\n",
    "    \"val_pe\",\"val_pb\",\"val_ps\",\"val_evebitda\",\n",
    "    \"log_mcap\",\n",
    "    \"grossmargin\",\"ebitdamargin\",\"netmargin\",\"roa\",\"roe\",\"currentratio\",\"assetturnover\",\n",
    "    \"rev_yoy\",\"eps_yoy\",\"asset_growth_yoy\",\"capex_assets\",\"inv_growth_yoy\",\"debt_growth_yoy\",\"lev_debt_assets\",\n",
    "    \"accruals_ni_ncfo_assets\",\"wc_chg_assets\",\n",
    "    \"divyield\",\n",
    "]\n",
    "factor_cols = [c for c in factor_cols_all if c in sf1_f.columns and _nonempty(sf1_f[c])]\n",
    "print(\"Dropped all-NA factors:\", [c for c in factor_cols_all if c not in factor_cols])\n",
    "\n",
    "sf1_f[\"avail_date\"] = pd.to_datetime(sf1_f[\"datekey\"], errors=\"coerce\").apply(_shift_avail)\n",
    "sf1_f = sf1_f.dropna(subset=[\"ticker\", \"avail_date\"])\n",
    "sf1_f = sf1_f.sort_values([\"ticker\", \"avail_date\", \"datekey\"], kind=\"mergesort\")\n",
    "sf1_f = sf1_f.drop_duplicates(subset=[\"ticker\", \"avail_date\"], keep=\"last\")\n",
    "\n",
    "if \"X_price\" in globals() and isinstance(X_price, pd.DataFrame):\n",
    "    X = X_price.copy()\n",
    "elif \"X\" in globals() and isinstance(X, pd.DataFrame):\n",
    "    X = X.copy()\n",
    "else:\n",
    "    raise NameError(\"Need X_price (or X) defined before SF1 alignment.\")\n",
    "\n",
    "left = X.index.to_frame(index=False).copy()\n",
    "left[\"date\"] = pd.to_datetime(left[\"date\"], errors=\"coerce\")\n",
    "left[\"ticker\"] = left[\"ticker\"].astype(str)\n",
    "left = left.dropna(subset=[\"date\", \"ticker\"])\n",
    "left = left.drop_duplicates(subset=[\"date\", \"ticker\"], keep=\"last\")\n",
    "left = left.sort_values([\"date\", \"ticker\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "right = sf1_f[[\"ticker\", \"avail_date\"] + factor_cols].copy()\n",
    "right[\"ticker\"] = right[\"ticker\"].astype(str)\n",
    "right[\"avail_date\"] = pd.to_datetime(right[\"avail_date\"], errors=\"coerce\")\n",
    "right = right.dropna(subset=[\"ticker\", \"avail_date\"])\n",
    "right = right.sort_values([\"avail_date\", \"ticker\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "merged = pd.merge_asof(\n",
    "    left,\n",
    "    right,\n",
    "    left_on=\"date\",\n",
    "    right_on=\"avail_date\",\n",
    "    by=\"ticker\",\n",
    "    direction=\"backward\",\n",
    "    allow_exact_matches=True,\n",
    ")\n",
    "\n",
    "lookahead = int((merged[\"avail_date\"].notna() & (merged[\"avail_date\"] > merged[\"date\"])).sum())\n",
    "print(\"Lookahead violations:\", lookahead)\n",
    "\n",
    "X_factors = merged.set_index([\"date\", \"ticker\"])[factor_cols].sort_index()\n",
    "for c in X_factors.columns:\n",
    "    X_factors[c] = X_factors[c].astype(\"float32\", copy=False)\n",
    "\n",
    "X_factors.to_parquet(SF1_ALIGNED_PATH)\n",
    "print(\"SF1 aligned saved:\", X_factors.shape, \"to\", SF1_ALIGNED_PATH)\n",
    "\n",
    "X_full = X.join(X_factors, how=\"left\").sort_index()\n",
    "print(\"X_full:\", X_full.shape)\n",
    "\n",
    "na_rate = X_factors.isna().mean().sort_values(ascending=False)\n",
    "print(\"NA rate (top 15):\")\n",
    "print(na_rate.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# winsorize and make X panel\n",
    "if \"X_full\" not in globals() or not isinstance(X_full, pd.DataFrame):\n",
    "    raise NameError(\"X_full not found. Run the SF1 alignment cell first so X_full exists.\")\n",
    "\n",
    "factor_cols = [c for c in X_full.columns if c not in X_price.columns] if \"X_price\" in globals() else [c for c in X_full.columns if c not in X.columns]\n",
    "factor_cols = [c for c in factor_cols if c in X_full.columns]\n",
    "\n",
    "panel = X_full.copy()\n",
    "\n",
    "def _winsorize_cs(df, lo=0.01, hi=0.99):\n",
    "    qlo = df.groupby(level=0).quantile(lo)\n",
    "    qhi = df.groupby(level=0).quantile(hi)\n",
    "\n",
    "    out = df.copy()\n",
    "    for c in df.columns:\n",
    "        lo_s = qlo[c]\n",
    "        hi_s = qhi[c]\n",
    "        out[c] = out[c].clip(lower=lo_s, upper=hi_s)\n",
    "    return out\n",
    "\n",
    "def _zscore_cs(df):\n",
    "    mu = df.groupby(level=0).transform(\"mean\")\n",
    "    sd = df.groupby(level=0).transform(lambda x: x.std(ddof=0))\n",
    "    return (df - mu) / sd.replace(0, np.nan)\n",
    "\n",
    "def _add_missing_indicators(df):\n",
    "    miss = df.isna().astype(\"int8\")\n",
    "    miss.columns = [f\"{c}__missing\" for c in miss.columns]\n",
    "    return miss\n",
    "\n",
    "base_cols = [c for c in panel.columns if c not in factor_cols]\n",
    "Xf = panel[factor_cols].astype(\"float32\").copy()\n",
    "\n",
    "coverage = Xf.notna().groupby(level=0).mean()\n",
    "mean_coverage = coverage.mean().sort_values()\n",
    "print(\"Mean factor coverage (bottom 10):\")\n",
    "print(mean_coverage.head(10))\n",
    "\n",
    "min_daily_coverage = 0.05\n",
    "keep_factors = mean_coverage[mean_coverage >= min_daily_coverage].index.tolist()\n",
    "drop_factors = [c for c in factor_cols if c not in keep_factors]\n",
    "print(\"Dropping low-coverage factors:\", drop_factors)\n",
    "\n",
    "Xf = Xf[keep_factors]\n",
    "\n",
    "winsor_lo = 0.01\n",
    "winsor_hi = 0.99\n",
    "Xf_w = _winsorize_cs(Xf, lo=winsor_lo, hi=winsor_hi)\n",
    "\n",
    "Xf_z = _zscore_cs(Xf_w)\n",
    "\n",
    "miss_ind = _add_missing_indicators(Xf_z)\n",
    "\n",
    "X_panel = pd.concat([panel[base_cols], Xf_z, miss_ind], axis=1).sort_index()\n",
    "\n",
    "print(\"X_panel:\", X_panel.shape)\n",
    "print(\"Example columns:\", X_panel.columns[:15].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish 3 month total return for backtest\n",
    "\n",
    "if \"px_stocks\" not in globals() or not isinstance(px_stocks, pd.DataFrame):\n",
    "    raise NameError(\"px_stocks not found. Load SEP first so px_stocks exists with columns ['ticker','date', ...].\")\n",
    "\n",
    "if \"X_panel\" not in globals() or not isinstance(X_panel, pd.DataFrame):\n",
    "    raise NameError(\"X_panel not found. Run your winsorize/zscore chunk first so X_panel exists with MultiIndex (date, ticker).\")\n",
    "\n",
    "px = px_stocks.copy()\n",
    "px[\"date\"] = pd.to_datetime(px[\"date\"], errors=\"coerce\")\n",
    "px[\"ticker\"] = px[\"ticker\"].astype(str)\n",
    "\n",
    "close_col = \"close\" if \"close\" in px.columns else (\"Close\" if \"Close\" in px.columns else None)\n",
    "if close_col is None:\n",
    "    raise KeyError(f\"Couldn't find a close column in px_stocks. Columns: {px.columns.tolist()}\")\n",
    "\n",
    "px = px.dropna(subset=[\"ticker\", \"date\", close_col]).sort_values([\"ticker\", \"date\"], kind=\"mergesort\")\n",
    "\n",
    "px_wide = px.pivot(index=\"date\", columns=\"ticker\", values=close_col).sort_index()\n",
    "px_wide = px_wide.ffill()\n",
    "\n",
    "h = 63  # ~3 months trading days\n",
    "fwd_ret = px_wide.shift(-h) / px_wide - 1.0\n",
    "\n",
    "y = fwd_ret.stack(dropna=False)\n",
    "y.index.names = [\"date\", \"ticker\"]\n",
    "y.name = \"ret_3m_fwd\"\n",
    "\n",
    "panel_index = X_panel.index\n",
    "y = y.reindex(panel_index)\n",
    "\n",
    "mask = y.notna()\n",
    "print(\"y coverage:\", float(mask.mean()), \"non-NA:\", int(mask.sum()), \"total:\", int(mask.size))\n",
    "print(\"y sample:\", y.dropna().iloc[:5].to_dict())\n",
    "\n",
    "y_ret3m = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"X_panel\" not in globals() or not isinstance(X_panel, pd.DataFrame):\n",
    "    raise NameError(\"X_panel not found.\")\n",
    "if \"y_ret3m\" not in globals():\n",
    "    raise NameError(\"y_ret3m not found.\")\n",
    "\n",
    "dates = pd.to_datetime(X_panel.index.get_level_values(\"date\")).unique()\n",
    "dates = pd.DatetimeIndex(sorted(dates))\n",
    "\n",
    "q_end_dates = pd.DatetimeIndex(sorted(dates[dates.is_quarter_end]))\n",
    "\n",
    "print(\"Quarter-end decision dates:\", len(q_end_dates))\n",
    "print(\"First / last q-end:\", q_end_dates.min(), q_end_dates.max())\n",
    "\n",
    "train_q = 32\n",
    "val_q = 8\n",
    "test_q = 12\n",
    "step_q = 4\n",
    "\n",
    "folds = []\n",
    "i = 0\n",
    "while True:\n",
    "    train_start = i\n",
    "    train_end = train_start + train_q\n",
    "    val_end = train_end + val_q\n",
    "    test_end = val_end + test_q\n",
    "\n",
    "    if test_end > len(q_end_dates):\n",
    "        break\n",
    "\n",
    "    folds.append(\n",
    "        {\n",
    "            \"train\": q_end_dates[train_start:train_end],\n",
    "            \"val\": q_end_dates[train_end:val_end],\n",
    "            \"test\": q_end_dates[val_end:test_end],\n",
    "        }\n",
    "    )\n",
    "    i += step_q\n",
    "\n",
    "for k, f in enumerate(folds, 1):\n",
    "    print(f\"\\nFold {k}\")\n",
    "    print(\"  Train:\", f[\"train\"][0].date(), \"->\", f[\"train\"][-1].date(), f\"({len(f['train'])} q)\")\n",
    "    print(\"  Val:  \", f[\"val\"][0].date(), \"->\", f[\"val\"][-1].date(), f\"({len(f['val'])} q)\")\n",
    "    print(\"  Test: \", f[\"test\"][0].date(), \"->\", f[\"test\"][-1].date(), f\"({len(f['test'])} q)\")\n",
    "\n",
    "mask_q = X_panel.index.get_level_values(\"date\").isin(q_end_dates)\n",
    "Xq = X_panel.loc[mask_q].copy()\n",
    "yq = y_ret3m.reindex(Xq.index)\n",
    "\n",
    "print(\"\\nQuarterly X shape:\", Xq.shape)\n",
    "print(\"Quarterly y coverage:\", float(yq.notna().mean()))\n",
    "\n",
    "fold_datasets = []\n",
    "for k, f in enumerate(folds, 1):\n",
    "    d = Xq.index.get_level_values(\"date\")\n",
    "    train_mask = d.isin(f[\"train\"])\n",
    "    val_mask = d.isin(f[\"val\"])\n",
    "    test_mask = d.isin(f[\"test\"])\n",
    "\n",
    "    fold_datasets.append(\n",
    "        {\n",
    "            \"fold\": k,\n",
    "            \"train_dates\": f[\"train\"],\n",
    "            \"val_dates\": f[\"val\"],\n",
    "            \"test_dates\": f[\"test\"],\n",
    "            \"X_train\": Xq.loc[train_mask],\n",
    "            \"y_train\": yq.loc[train_mask],\n",
    "            \"X_val\": Xq.loc[val_mask],\n",
    "            \"y_val\": yq.loc[val_mask],\n",
    "            \"X_test\": Xq.loc[test_mask],\n",
    "            \"y_test\": yq.loc[test_mask],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"\\nFold {k} rows\")\n",
    "    print(\"  Train rows:\", int(train_mask.sum()), \"y non-NA:\", int(yq.loc[train_mask].notna().sum()))\n",
    "    print(\"  Val rows:  \", int(val_mask.sum()), \"y non-NA:\", int(yq.loc[val_mask].notna().sum()))\n",
    "    print(\"  Test rows: \", int(test_mask.sum()), \"y non-NA:\", int(yq.loc[test_mask].notna().sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backtest\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "if \"fold_datasets\" not in globals() or len(fold_datasets) == 0:\n",
    "    raise NameError(\"fold_datasets not found. Run the combined folds+datasets cell first.\")\n",
    "\n",
    "if \"Xq\" not in globals() or \"yq\" not in globals():\n",
    "    raise NameError(\"Xq/yq not found. Run the combined folds+datasets cell first.\")\n",
    "\n",
    "SEED = int(globals().get(\"SEED\", 42))\n",
    "N_LONG = int(globals().get(\"N_LONG\", 50))\n",
    "N_SHORT = int(globals().get(\"N_SHORT\", 50))\n",
    "COST_BPS_ONE_WAY = float(globals().get(\"COST_BPS_ONE_WAY\", 0.0))\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "def _cs_zscore_series(y_s):\n",
    "    df = y_s.rename(\"y\").reset_index()\n",
    "    m = df.groupby(\"date\")[\"y\"].transform(\"mean\")\n",
    "    s = df.groupby(\"date\")[\"y\"].transform(\"std\").replace(0.0, np.nan)\n",
    "    out = (df[\"y\"] - m) / s\n",
    "    out.index = pd.MultiIndex.from_frame(df[[\"date\", \"ticker\"]])\n",
    "    out.index.names = [\"date\", \"ticker\"]\n",
    "    return out\n",
    "\n",
    "def _spearman_ic(pred_s, y_s):\n",
    "    df = pd.DataFrame({\"p\": pred_s, \"y\": y_s}).dropna()\n",
    "    if df.empty:\n",
    "        return np.nan\n",
    "    return df[\"p\"].rank().corr(df[\"y\"].rank())\n",
    "\n",
    "def _best_ntree(booster):\n",
    "    try:\n",
    "        a = booster.attributes()\n",
    "        if \"best_iteration\" in a:\n",
    "            return int(a[\"best_iteration\"]) + 1\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _stable_top_bottom(pred_s, n_long, n_short):\n",
    "    s = pred_s.dropna()\n",
    "    if s.empty:\n",
    "        return [], []\n",
    "    idx = s.index.astype(str).to_numpy()\n",
    "    val = s.to_numpy()\n",
    "    order_long = np.lexsort((idx, -val))\n",
    "    order_short = np.lexsort((idx, val))\n",
    "    longs = s.index.values[order_long][:n_long].tolist()\n",
    "    shorts = s.index.values[order_short][:n_short].tolist()\n",
    "    return longs, shorts\n",
    "\n",
    "base_params = {\n",
    "    \"max_depth\": 4,\n",
    "    \"eta\": 0.03,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"lambda\": 1.0,\n",
    "    \"alpha\": 0.0,\n",
    "    \"seed\": SEED,\n",
    "    \"nthread\": 1,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"predictor\": \"cpu_predictor\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "}\n",
    "\n",
    "feat_cols = list(Xq.columns)\n",
    "tickers_all = pd.Index(sorted(Xq.index.get_level_values(\"ticker\").astype(str).unique().tolist()))\n",
    "prev_w = pd.Series(0.0, index=tickers_all)\n",
    "\n",
    "rows_fit = []\n",
    "rows_perf = []\n",
    "rows_w = []\n",
    "rows_hold = []\n",
    "\n",
    "tasks = []\n",
    "for fd in fold_datasets:\n",
    "    test_dates = pd.Index(sorted(fd[\"X_test\"].index.get_level_values(\"date\").unique()))\n",
    "    for d in test_dates:\n",
    "        tasks.append((int(fd[\"fold\"]), pd.Timestamp(d)))\n",
    "\n",
    "pbar = tqdm(tasks, desc=\"Quarterly OOS quarters\", unit=\"q\", mininterval=0.5)\n",
    "\n",
    "fold_map = {int(fd[\"fold\"]): fd for fd in fold_datasets}\n",
    "trained_models = {}\n",
    "\n",
    "for fold_k, sig in pbar:\n",
    "    fd = fold_map[fold_k]\n",
    "\n",
    "    if fold_k not in trained_models:\n",
    "        X_train = fd[\"X_train\"].sort_index()\n",
    "        y_train = fd[\"y_train\"].reindex(X_train.index)\n",
    "\n",
    "        X_val = fd[\"X_val\"].sort_index()\n",
    "        y_val = fd[\"y_val\"].reindex(X_val.index)\n",
    "\n",
    "        y_train_cs = _cs_zscore_series(y_train)\n",
    "        y_val_cs = _cs_zscore_series(y_val)\n",
    "\n",
    "        mtr = y_train_cs.notna()\n",
    "        mva = y_val_cs.notna()\n",
    "\n",
    "        Xtr = X_train.loc[mtr, feat_cols].to_numpy(dtype=np.float32, copy=False)\n",
    "        ytr = y_train_cs.loc[mtr].to_numpy(dtype=np.float32, copy=False)\n",
    "        Xva = X_val.loc[mva, feat_cols].to_numpy(dtype=np.float32, copy=False)\n",
    "        yva = y_val_cs.loc[mva].to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "        if len(ytr) == 0 or len(yva) == 0:\n",
    "            trained_models[fold_k] = None\n",
    "            continue\n",
    "\n",
    "        dtr = xgb.DMatrix(Xtr, label=ytr, feature_names=feat_cols)\n",
    "        dva = xgb.DMatrix(Xva, label=yva, feature_names=feat_cols)\n",
    "\n",
    "        evals_result = {}\n",
    "        booster = xgb.train(\n",
    "            params=base_params,\n",
    "            dtrain=dtr,\n",
    "            num_boost_round=5000,\n",
    "            evals=[(dva, \"val\")],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=False,\n",
    "            evals_result=evals_result,\n",
    "        )\n",
    "\n",
    "        nt = _best_ntree(booster)\n",
    "        try:\n",
    "            n_trained = int(booster.num_boosted_rounds())\n",
    "        except Exception:\n",
    "            n_trained = np.nan\n",
    "\n",
    "        val_rmse_path = evals_result.get(\"val\", {}).get(\"rmse\", [])\n",
    "        val_rmse_best = float(np.min(val_rmse_path)) if len(val_rmse_path) else np.nan\n",
    "        val_rmse_last = float(val_rmse_path[-1]) if len(val_rmse_path) else np.nan\n",
    "\n",
    "        val_pred = booster.predict(dva)\n",
    "        rmse_model = float(np.sqrt(np.mean((val_pred - yva) ** 2)))\n",
    "        rmse_zero = float(np.sqrt(np.mean((0.0 - yva) ** 2)))\n",
    "        rmse_improve = rmse_zero - rmse_model\n",
    "        ic = _spearman_ic(pd.Series(val_pred), pd.Series(yva))\n",
    "\n",
    "        rows_fit.append(\n",
    "            {\n",
    "                \"fold\": fold_k,\n",
    "                \"trained_rounds\": n_trained,\n",
    "                \"best_trees\": int(nt) if nt is not None else np.nan,\n",
    "                \"val_rmse_best\": val_rmse_best,\n",
    "                \"val_rmse_last\": val_rmse_last,\n",
    "                \"rmse_model\": rmse_model,\n",
    "                \"rmse_zero\": rmse_zero,\n",
    "                \"rmse_improve\": rmse_improve,\n",
    "                \"val_ic\": float(ic) if ic == ic else np.nan,\n",
    "                \"train_rows\": int(len(ytr)),\n",
    "                \"val_rows\": int(len(yva)),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        trained_models[fold_k] = {\n",
    "            \"booster\": booster,\n",
    "            \"nt\": nt,\n",
    "            \"n_trained\": n_trained,\n",
    "            \"val_ic\": float(ic) if ic == ic else np.nan,\n",
    "            \"rmse_improve\": rmse_improve,\n",
    "        }\n",
    "\n",
    "    model_pack = trained_models.get(fold_k)\n",
    "    if model_pack is None:\n",
    "        continue\n",
    "\n",
    "    booster = model_pack[\"booster\"]\n",
    "    nt = model_pack[\"nt\"]\n",
    "    n_trained = model_pack[\"n_trained\"]\n",
    "    ic = model_pack[\"val_ic\"]\n",
    "    rmse_improve = model_pack[\"rmse_improve\"]\n",
    "\n",
    "    X_test = fd[\"X_test\"].sort_index()\n",
    "    y_test = fd[\"y_test\"].reindex(X_test.index)\n",
    "\n",
    "    try:\n",
    "        Xsig = X_test.loc[(sig, slice(None)), feat_cols]\n",
    "        ysig = y_test.loc[(sig, slice(None))]\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "    if Xsig.empty:\n",
    "        continue\n",
    "\n",
    "    Xsig = Xsig.sort_index()\n",
    "    Xsig_np = Xsig.to_numpy(dtype=np.float32, copy=False)\n",
    "    pred = pd.Series(\n",
    "        booster.predict(xgb.DMatrix(Xsig_np, feature_names=feat_cols)),\n",
    "        index=Xsig.index.get_level_values(\"ticker\").astype(str),\n",
    "    )\n",
    "\n",
    "    ysig = ysig.copy()\n",
    "    ysig.index = ysig.index.get_level_values(\"ticker\").astype(str)\n",
    "\n",
    "    universe0 = pred.index.intersection(ysig.index)\n",
    "\n",
    "    MIN_PRICE = float(globals().get(\"MIN_PRICE\", 5.0))\n",
    "    MIN_DVOL_20 = float(globals().get(\"MIN_DVOL_20\", 1e6))\n",
    "    \n",
    "    core = Xsig.copy()\n",
    "    core.index = core.index.get_level_values(\"ticker\").astype(str)\n",
    "    \n",
    "    px_ok = core[\"px\"] >= MIN_PRICE if \"px\" in core.columns else pd.Series(True, index=core.index)\n",
    "    liq_ok = core[\"dvol_20\"] >= MIN_DVOL_20 if \"dvol_20\" in core.columns else pd.Series(True, index=core.index)\n",
    "    \n",
    "    universe = [t for t in universe0 if bool(px_ok.get(t, False)) and bool(liq_ok.get(t, False))]\n",
    "    \n",
    "    pred_u = pred.loc[universe].dropna()\n",
    "    y_u = ysig.loc[universe]\n",
    "    \n",
    "    if len(pred_u) < (N_LONG + N_SHORT):\n",
    "        continue\n",
    "    \n",
    "    longs, shorts = _stable_top_bottom(pred_u, N_LONG, N_SHORT)\n",
    "    if len(longs) == 0 or len(shorts) == 0:\n",
    "        continue\n",
    "\n",
    "    w = pd.Series(0.0, index=tickers_all)\n",
    "    w.loc[longs] = 1.0 / len(longs)\n",
    "    w.loc[shorts] = -1.0 / len(shorts)\n",
    "\n",
    "    held = longs + shorts\n",
    "    y_held = y_u.reindex(held).astype(\"float64\").dropna()\n",
    "    if y_held.empty:\n",
    "        continue\n",
    "\n",
    "    w_held = w.reindex(y_held.index).astype(\"float64\")\n",
    "    gross = float((w_held * y_held).sum())\n",
    "\n",
    "    turnover = 0.5 * float((w - prev_w).abs().sum())\n",
    "    cost = turnover * (COST_BPS_ONE_WAY / 1e4)\n",
    "    net = gross - cost\n",
    "\n",
    "    rows_perf.append(\n",
    "        {\n",
    "            \"fold\": fold_k,\n",
    "            \"signal_date\": pd.Timestamp(sig),\n",
    "            \"gross_ret\": gross,\n",
    "            \"net_ret\": net,\n",
    "            \"turnover\": turnover,\n",
    "            \"n_universe\": int(len(pred_u)),\n",
    "            \"trained_rounds\": n_trained,\n",
    "            \"best_trees\": int(nt) if nt is not None else np.nan,\n",
    "            \"val_ic\": float(ic) if ic == ic else np.nan,\n",
    "            \"rmse_improve\": rmse_improve,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    w_sig = w[w != 0.0]\n",
    "    for tk, ww in w_sig.items():\n",
    "        rows_w.append({\"fold\": fold_k, \"signal_date\": pd.Timestamp(sig), \"ticker\": str(tk), \"weight\": float(ww)})\n",
    "\n",
    "    for tk in y_held.index:\n",
    "        side = \"LONG\" if w.loc[tk] > 0 else \"SHORT\"\n",
    "        rows_hold.append(\n",
    "            {\n",
    "                \"fold\": fold_k,\n",
    "                \"signal_date\": pd.Timestamp(sig),\n",
    "                \"ticker\": str(tk),\n",
    "                \"side\": side,\n",
    "                \"weight\": float(w.loc[tk]),\n",
    "                \"asset_return\": float(y_held.loc[tk]),\n",
    "                \"pred\": float(pred_u.loc[tk]) if tk in pred_u.index else np.nan,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    prev_w = w\n",
    "    pbar.set_postfix_str(f\"fold={fold_k} date={sig.date()}\")\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "perf = pd.DataFrame(rows_perf).sort_values([\"signal_date\", \"fold\"]).set_index(\"signal_date\")\n",
    "fit_diag = pd.DataFrame(rows_fit).sort_values([\"fold\"]).reset_index(drop=True)\n",
    "weights = pd.DataFrame(rows_w).sort_values([\"signal_date\", \"fold\", \"ticker\"]).reset_index(drop=True)\n",
    "holdings = pd.DataFrame(rows_hold).sort_values([\"signal_date\", \"fold\", \"side\", \"ticker\"]).reset_index(drop=True)\n",
    "\n",
    "def _perf_stats(r):\n",
    "    r = pd.Series(r).dropna()\n",
    "    if r.empty:\n",
    "        return pd.Series(dtype=\"float64\")\n",
    "    ann_factor = 4.0\n",
    "    n = float(len(r))\n",
    "    equity = (1.0 + r).cumprod()\n",
    "    ann_ret = equity.iloc[-1] ** (ann_factor / n) - 1.0\n",
    "    ann_vol = r.std(ddof=0) * np.sqrt(ann_factor)\n",
    "    sharpe = (r.mean() / r.std(ddof=0) * np.sqrt(ann_factor)) if r.std(ddof=0) != 0 else np.nan\n",
    "    running_max = equity.cummax()\n",
    "    dd = equity / running_max - 1.0\n",
    "    mdd = dd.min()\n",
    "    hit = (r > 0).mean()\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"n_periods\": n,\n",
    "            \"ann_return\": float(ann_ret),\n",
    "            \"ann_vol\": float(ann_vol),\n",
    "            \"sharpe\": float(sharpe) if sharpe == sharpe else np.nan,\n",
    "            \"max_drawdown\": float(mdd),\n",
    "            \"hit_rate\": float(hit),\n",
    "            \"avg_period_return\": float(r.mean()),\n",
    "            \"median_period_return\": float(r.median()),\n",
    "            \"cum_return\": float(equity.iloc[-1] - 1.0),\n",
    "        }\n",
    "    )\n",
    "\n",
    "stats_net = _perf_stats(perf[\"net_ret\"]) if \"net_ret\" in perf.columns else pd.Series(dtype=\"float64\")\n",
    "stats_gross = _perf_stats(perf[\"gross_ret\"]) if \"gross_ret\" in perf.columns else pd.Series(dtype=\"float64\")\n",
    "\n",
    "print(\"perf rows:\", len(perf), \"fit folds:\", len(fit_diag), \"weights rows:\", len(weights), \"holdings rows:\", len(holdings))\n",
    "print(\"\\nBacktest stats (NET):\")\n",
    "print(stats_net)\n",
    "print(\"\\nBacktest stats (GROSS):\")\n",
    "print(stats_gross)\n",
    "\n",
    "perf.tail(), fit_diag, holdings.head(), weights.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IC time series\n",
    "\n",
    "if \"holdings\" not in globals():\n",
    "    raise NameError(\"holdings not found.\")\n",
    "\n",
    "ic_rows = []\n",
    "for (d, f), grp in holdings.groupby([\"signal_date\",\"fold\"]):\n",
    "    if grp.shape[0] < 50:\n",
    "        continue\n",
    "    ic = grp[\"pred\"].rank().corr(grp[\"asset_return\"].rank())\n",
    "    ic_rows.append({\"signal_date\": d, \"fold\": f, \"ic\": ic})\n",
    "\n",
    "ic_df = pd.DataFrame(ic_rows).sort_values(\"signal_date\")\n",
    "print(\"Mean IC:\", ic_df[\"ic\"].mean())\n",
    "print(\"IC std:\", ic_df[\"ic\"].std())\n",
    "print(\"IC t-stat:\", ic_df[\"ic\"].mean() / (ic_df[\"ic\"].std() / np.sqrt(len(ic_df))))\n",
    "\n",
    "ic_df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decile test\n",
    "\n",
    "decile_rows = []\n",
    "\n",
    "for (d, f), grp in holdings.groupby([\"signal_date\",\"fold\"]):\n",
    "    grp = grp.dropna(subset=[\"pred\",\"asset_return\"])\n",
    "    if len(grp) < 100:\n",
    "        continue\n",
    "\n",
    "    grp[\"decile\"] = pd.qcut(grp[\"pred\"], 10, labels=False, duplicates=\"drop\")\n",
    "    dec = grp.groupby(\"decile\")[\"asset_return\"].mean()\n",
    "\n",
    "    for k, v in dec.items():\n",
    "        decile_rows.append({\"signal_date\": d, \"fold\": f, \"decile\": int(k), \"ret\": v})\n",
    "\n",
    "dec_df = pd.DataFrame(decile_rows)\n",
    "dec_summary = dec_df.groupby(\"decile\")[\"ret\"].mean()\n",
    "print(dec_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long short spread\n",
    "ls_rows = []\n",
    "\n",
    "for (d, f), grp in holdings.groupby([\"signal_date\",\"fold\"]):\n",
    "    grp = grp.dropna(subset=[\"pred\",\"asset_return\"])\n",
    "    if len(grp) < 100:\n",
    "        continue\n",
    "\n",
    "    grp[\"decile\"] = pd.qcut(grp[\"pred\"], 10, labels=False, duplicates=\"drop\")\n",
    "    top = grp[grp[\"decile\"] == grp[\"decile\"].max()][\"asset_return\"].mean()\n",
    "    bot = grp[grp[\"decile\"] == grp[\"decile\"].min()][\"asset_return\"].mean()\n",
    "\n",
    "    ls_rows.append({\n",
    "        \"signal_date\": d,\n",
    "        \"fold\": f,\n",
    "        \"ls_spread\": top - bot\n",
    "    })\n",
    "\n",
    "ls_df = pd.DataFrame(ls_rows)\n",
    "print(ls_df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_reset = perf.reset_index()\n",
    "perf_reset[\"year\"] = perf_reset[\"signal_date\"].dt.year\n",
    "\n",
    "print(\"Pre-2015:\")\n",
    "print(perf_reset[perf_reset[\"year\"] < 2015][\"net_ret\"].mean())\n",
    "\n",
    "print(\"2015\u20132019:\")\n",
    "print(perf_reset[(perf_reset[\"year\"] >= 2015) & (perf_reset[\"year\"] < 2020)][\"net_ret\"].mean())\n",
    "\n",
    "print(\"2020+ :\")\n",
    "print(perf_reset[perf_reset[\"year\"] >= 2020][\"net_ret\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = []\n",
    "\n",
    "for fd in fold_datasets:\n",
    "    k = fd[\"fold\"]\n",
    "    model = trained_models.get(k, {}).get(\"booster\")\n",
    "    if model is None:\n",
    "        continue\n",
    "    imp = model.get_score(importance_type=\"gain\")\n",
    "    for f, v in imp.items():\n",
    "        importances.append({\"fold\": k, \"feature\": f, \"gain\": v})\n",
    "\n",
    "imp_df = pd.DataFrame(importances)\n",
    "imp_avg = imp_df.groupby(\"feature\")[\"gain\"].mean().sort_values(ascending=False)\n",
    "imp_avg.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _rank_corr(x, y):\n",
    "    if len(x) < 2:\n",
    "        return np.nan\n",
    "    x_rank = x.rank(method=\"average\")\n",
    "    y_rank = y.rank(method=\"average\")\n",
    "    return x_rank.corr(y_rank)\n",
    "\n",
    "\n",
    "def dedupe_live(perf, weights, holdings):\n",
    "    perf = perf.sort_values([\"signal_date\", \"fold\"])\n",
    "    latest = perf.groupby(\"signal_date\")[\"fold\"].idxmax()\n",
    "    perf_live = perf.loc[latest].reset_index(drop=True)\n",
    "\n",
    "    keep = perf_live[[\"signal_date\", \"fold\"]]\n",
    "    weights_live = weights.merge(keep, on=[\"signal_date\", \"fold\"], how=\"inner\")\n",
    "    holdings_live = holdings.merge(keep, on=[\"signal_date\", \"fold\"], how=\"inner\")\n",
    "\n",
    "    return {\n",
    "        \"perf_live\": perf_live,\n",
    "        \"weights_live\": weights_live,\n",
    "        \"holdings_live\": holdings_live,\n",
    "    }\n",
    "\n",
    "\n",
    "def ic_timeseries(panel):\n",
    "    rows = []\n",
    "    for keys, grp in panel.groupby([\"signal_date\", \"fold\"]):\n",
    "        d, f = keys\n",
    "        grp = grp.dropna(subset=[\"pred\", \"asset_return\"])\n",
    "        if len(grp) < 2:\n",
    "            continue\n",
    "        ic = _rank_corr(grp[\"pred\"], grp[\"asset_return\"])\n",
    "        rows.append({\"signal_date\": d, \"fold\": f, \"ic\": ic})\n",
    "\n",
    "    ic_df = pd.DataFrame(rows)\n",
    "    if not ic_df.empty:\n",
    "        mean_ic = ic_df[\"ic\"].mean()\n",
    "        t_stat = mean_ic / ic_df[\"ic\"].std(ddof=1) * np.sqrt(len(ic_df)) if len(ic_df) > 1 else np.nan\n",
    "        ic_df.attrs.update({\"mean_ic\": mean_ic, \"t_stat\": t_stat})\n",
    "    return ic_df\n",
    "\n",
    "\n",
    "def decile_curve(universe_panel):\n",
    "    rows = []\n",
    "    for keys, grp in universe_panel.groupby([\"signal_date\", \"fold\"]):\n",
    "        d, f = keys\n",
    "        grp = grp.dropna(subset=[\"pred\", \"asset_return\"])\n",
    "        if len(grp) < 10:\n",
    "            continue\n",
    "        grp = grp.copy()\n",
    "        grp[\"decile\"] = pd.qcut(grp[\"pred\"], 10, labels=False, duplicates=\"drop\")\n",
    "        deci = grp.groupby(\"decile\")[\"asset_return\"].mean()\n",
    "        for dec, val in deci.items():\n",
    "            rows.append({\"signal_date\": d, \"fold\": f, \"decile\": int(dec), \"ret\": float(val)})\n",
    "\n",
    "    decile_df = pd.DataFrame(rows)\n",
    "    summary = None\n",
    "    if not decile_df.empty:\n",
    "        summary = decile_df.groupby(\"decile\")[\"ret\"].mean().rename(\"mean_ret\")\n",
    "        top_bottom = summary.loc[summary.index.max()] - summary.loc[summary.index.min()]\n",
    "        summary = summary.to_frame()\n",
    "        summary.loc[\"top_bottom\", \"mean_ret\"] = top_bottom\n",
    "    return decile_df, summary\n",
    "\n",
    "\n",
    "def ls_spread_timeseries(universe_panel):\n",
    "    rows = []\n",
    "    for keys, grp in universe_panel.groupby([\"signal_date\", \"fold\"]):\n",
    "        d, f = keys\n",
    "        grp = grp.dropna(subset=[\"pred\", \"asset_return\"])\n",
    "        if len(grp) < 10:\n",
    "            continue\n",
    "        grp = grp.copy()\n",
    "        grp[\"decile\"] = pd.qcut(grp[\"pred\"], 10, labels=False, duplicates=\"drop\")\n",
    "        top = grp[grp[\"decile\"] == grp[\"decile\"].max()][\"asset_return\"].mean()\n",
    "        bot = grp[grp[\"decile\"] == grp[\"decile\"].min()][\"asset_return\"].mean()\n",
    "        rows.append({\"signal_date\": d, \"fold\": f, \"ls_spread\": top - bot})\n",
    "\n",
    "    ls_df = pd.DataFrame(rows)\n",
    "    if not ls_df.empty:\n",
    "        mean_ls = ls_df[\"ls_spread\"].mean()\n",
    "        t_stat = mean_ls / ls_df[\"ls_spread\"].std(ddof=1) * np.sqrt(len(ls_df)) if len(ls_df) > 1 else np.nan\n",
    "        ls_df.attrs.update({\"mean_ls\": mean_ls, \"t_stat\": t_stat})\n",
    "    return ls_df\n",
    "\n",
    "\n",
    "def turnover_report(perf_live):\n",
    "    df = perf_live.dropna(subset=[\"turnover\", \"net_ret\"]).copy()\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    df[\"turnover_q\"] = pd.qcut(df[\"turnover\"], 4, labels=False, duplicates=\"drop\")\n",
    "    report = df.groupby(\"turnover_q\").agg(\n",
    "        mean_turnover=(\"turnover\", \"mean\"),\n",
    "        mean_net_ret=(\"net_ret\", \"mean\"),\n",
    "        count=(\"net_ret\", \"count\"),\n",
    "    )\n",
    "    return report\n",
    "\n",
    "\n",
    "def cost_sensitivity(perf_inputs, cost_grid=(0, 5, 10, 20, 50)):\n",
    "    if \"gross_ret\" not in perf_inputs or \"turnover\" not in perf_inputs:\n",
    "        raise ValueError(\"perf_inputs must include gross_ret and turnover columns.\")\n",
    "    rows = []\n",
    "    for cost in cost_grid:\n",
    "        net = perf_inputs[\"gross_ret\"] - perf_inputs[\"turnover\"] * (cost / 10_000)\n",
    "        rows.append({\"cost_bps\": cost, \"mean_net_ret\": net.mean()})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def export_quarter_holdings_excel(holdings_live, date, path, n_long, n_short):\n",
    "    d = pd.Timestamp(date)\n",
    "    df = holdings_live[holdings_live[\"signal_date\"] == d]\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No holdings found for the requested date.\")\n",
    "    longs = df[df[\"side\"] == \"long\"].nlargest(n_long, \"weight\")\n",
    "    shorts = df[df[\"side\"] == \"short\"].nsmallest(n_short, \"weight\")\n",
    "\n",
    "    with pd.ExcelWriter(path) as writer:\n",
    "        longs[[\"ticker\", \"weight\", \"asset_return\"]].to_excel(writer, sheet_name=\"LONG\", index=False)\n",
    "        shorts[[\"ticker\", \"weight\", \"asset_return\"]].to_excel(writer, sheet_name=\"SHORT\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stats and export\n",
    "perf_reset = perf.reset_index().copy()\n",
    "perf_reset[\"signal_date\"] = pd.to_datetime(perf_reset[\"signal_date\"])\n",
    "perf_reset[\"fold\"] = perf_reset[\"fold\"].astype(int)\n",
    "\n",
    "live_outputs = dedupe_live(perf_reset, weights, holdings)\n",
    "perf_live = live_outputs[\"perf_live\"].set_index(\"signal_date\").sort_index()\n",
    "weights_live = live_outputs[\"weights_live\"].sort_values([\"signal_date\", \"fold\", \"ticker\"]).reset_index(drop=True)\n",
    "holdings_live = live_outputs[\"holdings_live\"].sort_values([\"signal_date\", \"fold\", \"side\", \"ticker\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"LIVE Backtest stats (NET):\")\n",
    "print(_perf_stats(perf_live[\"net_ret\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d2814-df49-461f-95e1-428b35fd6600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if \"holdings_live\" not in globals() or holdings_live.empty:\n",
    "    raise NameError(\"holdings_live not found. Run dedupe_live first.\")\n",
    "\n",
    "H = holdings_live.copy()\n",
    "H[\"signal_date\"] = pd.to_datetime(H[\"signal_date\"])\n",
    "H[\"pnl\"] = H[\"weight\"].astype(float) * H[\"asset_return\"].astype(float)\n",
    "H[\"win\"] = (H[\"pnl\"] > 0).astype(int)\n",
    "\n",
    "bets_by_q = (\n",
    "    H.groupby(\"signal_date\")\n",
    "    .agg(\n",
    "        n_bets=(\"ticker\", \"count\"),\n",
    "        win_rate=(\"win\", \"mean\"),\n",
    "        long_win_rate=(\"win\", lambda x: float(np.mean(x.loc[H.loc[x.index, \"side\"].eq(\"LONG\")])) if (H.loc[x.index, \"side\"].eq(\"LONG\")).any() else np.nan),\n",
    "        short_win_rate=(\"win\", lambda x: float(np.mean(x.loc[H.loc[x.index, \"side\"].eq(\"SHORT\")])) if (H.loc[x.index, \"side\"].eq(\"SHORT\")).any() else np.nan),\n",
    "        sum_pnl=(\"pnl\", \"sum\"),\n",
    "    )\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "print(\"n_bets value counts:\")\n",
    "print(bets_by_q[\"n_bets\"].value_counts().sort_index())\n",
    "\n",
    "display(bets_by_q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _max_date(obj, name):\n",
    "    if obj is None:\n",
    "        print(name, \"is None\")\n",
    "        return\n",
    "    if isinstance(obj, pd.DataFrame) or isinstance(obj, pd.Series):\n",
    "        if isinstance(obj.index, pd.MultiIndex):\n",
    "            d = pd.to_datetime(obj.index.get_level_values(\"date\")).max()\n",
    "        else:\n",
    "            d = pd.to_datetime(obj.index).max()\n",
    "        print(f\"{name} max date:\", d)\n",
    "    else:\n",
    "        print(name, \"type:\", type(obj))\n",
    "\n",
    "for nm in [\"stock_close\", \"X_price\", \"X_full\", \"Xq\", \"y_ret3m\", \"pairs_df\"]:\n",
    "    _max_date(globals().get(nm, None), nm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}