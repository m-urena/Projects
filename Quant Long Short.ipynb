{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1291474-6d0c-4fa0-bff6-b28548f40c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nasdaqdatalink\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "pd.set_option(\"mode.copy_on_write\", True)\n",
    "\n",
    "SEED = 7\n",
    "np.random.seed(SEED)\n",
    "\n",
    "START = \"2005-01-01\"\n",
    "END = \"2025-12-31\"\n",
    "\n",
    "MIN_PRICE = 5.0\n",
    "MIN_DVOL_20 = 5_000_000\n",
    "\n",
    "N_LONG = 50\n",
    "N_SHORT = 50\n",
    "\n",
    "COST_BPS_ONE_WAY = 5.0\n",
    "FUNDAMENTALS_LAG_TRADING_DAYS = 1\n",
    "\n",
    "OUTPUT_XLSX = \"monthly_holdings.xlsx\"\n",
    "CACHE_DIR = \"cache_sharadar\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "nasdaqdatalink.ApiConfig.api_key = \"vo3osJWr68eTVPawaV_B\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95fde9f7-f1c7-4589-a10d-cd09a1b8e32e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SFP', 'SPY')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#functions\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def _chunk(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "def _month_end_trading_days(trading_days_index):\n",
    "    s = pd.Series(trading_days_index, index=trading_days_index)\n",
    "    return s.groupby(s.index.to_period(\"M\")).max().sort_values().tolist()\n",
    "\n",
    "def _next_trading_day(trading_days_index, d):\n",
    "    d = pd.Timestamp(d)\n",
    "    pos = trading_days_index.searchsorted(d)\n",
    "    if pos >= len(trading_days_index) - 1:\n",
    "        return None\n",
    "    if trading_days_index[pos] == d:\n",
    "        return trading_days_index[pos + 1]\n",
    "    return trading_days_index[pos]\n",
    "\n",
    "def _shift_trading_days(trading_days_index, d, k):\n",
    "    d = pd.Timestamp(d)\n",
    "    pos = trading_days_index.searchsorted(d)\n",
    "    if pos >= len(trading_days_index):\n",
    "        return None\n",
    "    if trading_days_index[pos] != d:\n",
    "        pos -= 1\n",
    "    pos2 = pos + int(k)\n",
    "    if pos2 < 0 or pos2 >= len(trading_days_index):\n",
    "        return None\n",
    "    return trading_days_index[pos2]\n",
    "\n",
    "def _save_parquet(df, path):\n",
    "    df.to_parquet(path, index=False)\n",
    "\n",
    "def _load_parquet(path):\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def fetch_tickers_universe(table=\"SEP\"):\n",
    "    path = Path(CACHE_DIR) / f\"tickers_{table}.parquet\"\n",
    "    if path.exists():\n",
    "        return _load_parquet(path)\n",
    "\n",
    "    df = nasdaqdatalink.get_table(\"SHARADAR/TICKERS\", table=table, paginate=True)\n",
    "    if df is None or df.empty:\n",
    "        raise ValueError(\"SHARADAR/TICKERS returned no rows.\")\n",
    "    if \"ticker\" not in df.columns:\n",
    "        raise ValueError(\"SHARADAR/TICKERS did not return 'ticker'.\")\n",
    "\n",
    "    if \"category\" in df.columns:\n",
    "        df = df[df[\"category\"].isin([\"Domestic Common Stock\", \"Domestic Common Stock Primary\", \"Domestic Common Stock Secondary\"])]\n",
    "\n",
    "    df = df.dropna(subset=[\"ticker\"]).reset_index(drop=True)\n",
    "    _save_parquet(df, path)\n",
    "    return df\n",
    "\n",
    "def build_trade_universe(univ, start, end):\n",
    "    u = univ.copy()\n",
    "    start_dt = pd.Timestamp(start)\n",
    "    end_dt = pd.Timestamp(end)\n",
    "\n",
    "    if \"firstpricedate\" in u.columns:\n",
    "        u[\"firstpricedate\"] = pd.to_datetime(u[\"firstpricedate\"], errors=\"coerce\")\n",
    "    if \"lastpricedate\" in u.columns:\n",
    "        u[\"lastpricedate\"] = pd.to_datetime(u[\"lastpricedate\"], errors=\"coerce\")\n",
    "\n",
    "    if \"firstpricedate\" in u.columns and \"lastpricedate\" in u.columns:\n",
    "        u = u[(u[\"firstpricedate\"] <= end_dt) & (u[\"lastpricedate\"] >= start_dt)]\n",
    "\n",
    "    return u[\"ticker\"].dropna().unique().tolist()\n",
    "\n",
    "def fetch_sep_prices_yearly(tickers, start, end, columns=None, chunk_size=150, cache_name=\"sep_full\"):\n",
    "    start_dt = pd.Timestamp(start)\n",
    "    end_dt = pd.Timestamp(end)\n",
    "\n",
    "    if columns is None:\n",
    "        columns = [\"ticker\", \"date\", \"closeadj\", \"close\", \"volume\"]\n",
    "\n",
    "    path = Path(CACHE_DIR) / f\"{cache_name}_{start_dt.date()}_{end_dt.date()}_c{chunk_size}.parquet\"\n",
    "    if path.exists():\n",
    "        return _load_parquet(path)\n",
    "\n",
    "    years = range(start_dt.year, end_dt.year + 1)\n",
    "    out = []\n",
    "\n",
    "    for y in years:\n",
    "        w0 = max(pd.Timestamp(f\"{y}-01-01\"), start_dt)\n",
    "        w1 = min(pd.Timestamp(f\"{y}-12-31\"), end_dt)\n",
    "        if w0 > w1:\n",
    "            continue\n",
    "\n",
    "        w0s = str(w0.date())\n",
    "        w1s = str(w1.date())\n",
    "\n",
    "        for tk_chunk in _chunk(list(tickers), chunk_size):\n",
    "            df = nasdaqdatalink.get_table(\n",
    "                \"SHARADAR/SEP\",\n",
    "                ticker=tk_chunk,\n",
    "                date={\"gte\": w0s, \"lte\": w1s},\n",
    "                qopts={\"columns\": columns},\n",
    "                paginate=True\n",
    "            )\n",
    "            if df is not None and not df.empty:\n",
    "                out.append(df)\n",
    "\n",
    "    px = pd.concat(out, ignore_index=True) if out else pd.DataFrame()\n",
    "    if px.empty:\n",
    "        raise ValueError(\"SEP pull returned no rows. Check tickers/date range.\")\n",
    "\n",
    "    px[\"date\"] = pd.to_datetime(px[\"date\"])\n",
    "    _save_parquet(px, path)\n",
    "    return px\n",
    "\n",
    "def fetch_sfp_prices_yearly(tickers, start, end, columns=None, chunk_size=200, cache_name=\"sfp_full\"):\n",
    "    start_dt = pd.Timestamp(start)\n",
    "    end_dt = pd.Timestamp(end)\n",
    "\n",
    "    if columns is None:\n",
    "        columns = [\"ticker\", \"date\", \"closeadj\", \"close\", \"volume\"]\n",
    "\n",
    "    path = Path(CACHE_DIR) / f\"{cache_name}_{start_dt.date()}_{end_dt.date()}_c{chunk_size}.parquet\"\n",
    "    if path.exists():\n",
    "        return _load_parquet(path)\n",
    "\n",
    "    years = range(start_dt.year, end_dt.year + 1)\n",
    "    out = []\n",
    "\n",
    "    for y in years:\n",
    "        w0 = max(pd.Timestamp(f\"{y}-01-01\"), start_dt)\n",
    "        w1 = min(pd.Timestamp(f\"{y}-12-31\"), end_dt)\n",
    "        if w0 > w1:\n",
    "            continue\n",
    "\n",
    "        w0s = str(w0.date())\n",
    "        w1s = str(w1.date())\n",
    "\n",
    "        for tk_chunk in _chunk(list(tickers), chunk_size):\n",
    "            df = nasdaqdatalink.get_table(\n",
    "                \"SHARADAR/SFP\",\n",
    "                ticker=tk_chunk,\n",
    "                date={\"gte\": w0s, \"lte\": w1s},\n",
    "                qopts={\"columns\": columns},\n",
    "                paginate=True\n",
    "            )\n",
    "            if df is not None and not df.empty:\n",
    "                out.append(df)\n",
    "\n",
    "    px = pd.concat(out, ignore_index=True) if out else pd.DataFrame()\n",
    "    if px.empty:\n",
    "        raise ValueError(\"SFP pull returned no rows. Check entitlement/tickers/date range.\")\n",
    "\n",
    "    px[\"date\"] = pd.to_datetime(px[\"date\"])\n",
    "    _save_parquet(px, path)\n",
    "    return px\n",
    "\n",
    "def fetch_sf1_fundamentals_all_numeric(tickers, start, end, dimension=\"ART\", chunk_size=200, cache_name=\"sf1_allnum\"):\n",
    "    start_s = str(pd.Timestamp(start).date())\n",
    "    end_s = str(pd.Timestamp(end).date())\n",
    "\n",
    "    path = Path(CACHE_DIR) / f\"{cache_name}_{dimension}_{start_s}_{end_s}_c{chunk_size}.parquet\"\n",
    "    if path.exists():\n",
    "        return _load_parquet(path)\n",
    "\n",
    "    out = []\n",
    "    for tk_chunk in _chunk(list(tickers), chunk_size):\n",
    "        df = nasdaqdatalink.get_table(\n",
    "            \"SHARADAR/SF1\",\n",
    "            ticker=tk_chunk,\n",
    "            dimension=dimension,\n",
    "            datekey={\"gte\": start_s, \"lte\": end_s},\n",
    "            paginate=True\n",
    "        )\n",
    "        if df is not None and not df.empty:\n",
    "            out.append(df)\n",
    "\n",
    "    f = pd.concat(out, ignore_index=True) if out else pd.DataFrame()\n",
    "    if f.empty:\n",
    "        return f\n",
    "\n",
    "    f[\"datekey\"] = pd.to_datetime(f[\"datekey\"], errors=\"coerce\")\n",
    "    if \"calendardate\" in f.columns:\n",
    "        f[\"calendardate\"] = pd.to_datetime(f[\"calendardate\"], errors=\"coerce\")\n",
    "\n",
    "    keep_id = {\"ticker\", \"dimension\", \"datekey\", \"calendardate\"}\n",
    "    num_cols = [c for c in f.columns if c in keep_id or pd.api.types.is_numeric_dtype(f[c])]\n",
    "    f = f[num_cols]\n",
    "\n",
    "    _save_parquet(f, path)\n",
    "    return f\n",
    "\n",
    "def pick_benchmark(as_of_date):\n",
    "    d = str(pd.Timestamp(as_of_date).date())\n",
    "\n",
    "    for tk in [\"SPX\", \"^GSPC\"]:\n",
    "        try:\n",
    "            df = nasdaqdatalink.get_table(\n",
    "                \"SHARADAR/SEP\",\n",
    "                ticker=tk,\n",
    "                date={\"eq\": d},\n",
    "                qopts={\"columns\": [\"ticker\", \"date\", \"close\", \"closeadj\"]},\n",
    "                paginate=False\n",
    "            )\n",
    "            if df is not None and not df.empty:\n",
    "                return (\"SEP\", tk)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    for tk in [\"SPY\", \"IVV\", \"VOO\"]:\n",
    "        try:\n",
    "            df = nasdaqdatalink.get_table(\n",
    "                \"SHARADAR/SFP\",\n",
    "                ticker=tk,\n",
    "                date={\"eq\": d},\n",
    "                qopts={\"columns\": [\"ticker\", \"date\", \"close\", \"closeadj\"]},\n",
    "                paginate=False\n",
    "            )\n",
    "            if df is not None and not df.empty:\n",
    "                return (\"SFP\", tk)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return (\"SFP\", \"SPY\")\n",
    "\n",
    "univ = fetch_tickers_universe(table=\"SEP\")\n",
    "tickers = build_trade_universe(univ, START, END)\n",
    "\n",
    "BENCH_SOURCE, BENCH_TICKER = pick_benchmark(END)\n",
    "BENCH_SOURCE, BENCH_TICKER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- backtest config + modular functions (long/short) ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "CONFIG = dict(\n",
    "    SEED=7,\n",
    "    N_LONG=50,\n",
    "    N_SHORT=50,\n",
    "    TRAIN_Q=12,\n",
    "    VAL_Q=4,\n",
    "    TEST_Q=4,\n",
    "    STEP_Q=4,\n",
    "    MIN_PRICE=5.0,\n",
    "    MIN_DVOL_20=1_000_000,\n",
    "    COST_BPS_ONE_WAY=5.0,\n",
    "    RET_CLIP_LOWER=-0.8,\n",
    "    RET_CLIP_UPPER=1.5,\n",
    "    USE_RET_CLIP=False,\n",
    ")\n",
    "\n",
    "\n",
    "def make_folds(q_end_dates, train_q, val_q, test_q, step_q):\n",
    "    q_end = pd.DatetimeIndex(sorted(pd.to_datetime(q_end_dates)))\n",
    "    total = train_q + val_q + test_q\n",
    "    folds = []\n",
    "    k = 0\n",
    "    for i in range(0, len(q_end) - total + 1, step_q):\n",
    "        train_dates = q_end[i : i + train_q]\n",
    "        val_dates = q_end[i + train_q : i + train_q + val_q]\n",
    "        test_dates = q_end[i + train_q + val_q : i + total]\n",
    "        folds.append(\n",
    "            {\n",
    "                \"fold\": k,\n",
    "                \"train_dates\": list(train_dates),\n",
    "                \"val_dates\": list(val_dates),\n",
    "                \"test_dates\": list(test_dates),\n",
    "            }\n",
    "        )\n",
    "        k += 1\n",
    "    return folds\n",
    "\n",
    "\n",
    "def build_fold_datasets(Xq, y_ret3m, folds):\n",
    "    if not isinstance(Xq.index, pd.MultiIndex):\n",
    "        raise ValueError(\"Xq must use a MultiIndex (date, ticker).\")\n",
    "    if not isinstance(y_ret3m.index, pd.MultiIndex):\n",
    "        raise ValueError(\"y_ret3m must use a MultiIndex (date, ticker).\")\n",
    "\n",
    "    Xq = Xq.sort_index()\n",
    "    y_ret3m = y_ret3m.sort_index()\n",
    "\n",
    "    datasets = []\n",
    "    for fold in folds:\n",
    "        train_dates = fold[\"train_dates\"]\n",
    "        val_dates = fold[\"val_dates\"]\n",
    "        test_dates = fold[\"test_dates\"]\n",
    "\n",
    "        X_train = Xq.loc[(train_dates, slice(None))].sort_index()\n",
    "        y_train = y_ret3m.loc[(train_dates, slice(None))].reindex(X_train.index)\n",
    "\n",
    "        X_val = Xq.loc[(val_dates, slice(None))].sort_index()\n",
    "        y_val = y_ret3m.loc[(val_dates, slice(None))].reindex(X_val.index)\n",
    "\n",
    "        X_test = Xq.loc[(test_dates, slice(None))].sort_index()\n",
    "        y_test = y_ret3m.loc[(test_dates, slice(None))].reindex(X_test.index)\n",
    "\n",
    "        datasets.append(\n",
    "            {\n",
    "                \"fold\": fold[\"fold\"],\n",
    "                \"X_train\": X_train,\n",
    "                \"y_train\": y_train,\n",
    "                \"X_val\": X_val,\n",
    "                \"y_val\": y_val,\n",
    "                \"X_test\": X_test,\n",
    "                \"y_test\": y_test,\n",
    "            }\n",
    "        )\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def _zscore_by_date(y):\n",
    "    def _z(s):\n",
    "        mu = s.mean()\n",
    "        sigma = s.std(ddof=0)\n",
    "        if sigma == 0 or np.isnan(sigma):\n",
    "            return pd.Series(0.0, index=s.index)\n",
    "        return (s - mu) / sigma\n",
    "\n",
    "    return y.groupby(level=0).transform(_z)\n",
    "\n",
    "\n",
    "def _rank_corr(x, y):\n",
    "    if len(x) < 2:\n",
    "        return np.nan\n",
    "    x_rank = x.rank(method=\"average\")\n",
    "    y_rank = y.rank(method=\"average\")\n",
    "    return x_rank.corr(y_rank)\n",
    "\n",
    "\n",
    "def train_xgb_fold(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    feat_cols,\n",
    "    params,\n",
    "    early_stop=50,\n",
    "    num_rounds=500,\n",
    "):\n",
    "    X_train = X_train.sort_index()\n",
    "    X_val = X_val.sort_index()\n",
    "\n",
    "    y_train_z = _zscore_by_date(y_train)\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train[feat_cols], label=y_train_z)\n",
    "    dval = xgb.DMatrix(X_val[feat_cols], label=_zscore_by_date(y_val))\n",
    "\n",
    "    p = dict(params)\n",
    "    p.setdefault(\"nthread\", 1)\n",
    "    p.setdefault(\"tree_method\", \"hist\")\n",
    "    p.setdefault(\"seed\", CONFIG[\"SEED\"])\n",
    "\n",
    "    evals_result = {}\n",
    "    booster = xgb.train(\n",
    "        p,\n",
    "        dtrain,\n",
    "        num_boost_round=num_rounds,\n",
    "        evals=[(dval, \"val\")],\n",
    "        early_stopping_rounds=early_stop,\n",
    "        evals_result=evals_result,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "\n",
    "    best_iteration = booster.best_iteration\n",
    "    trained_rounds = booster.best_iteration + 1\n",
    "    val_rmse = evals_result[\"val\"][\"rmse\"]\n",
    "    val_rmse_best = float(val_rmse[best_iteration]) if val_rmse else np.nan\n",
    "    rmse_improve = float(val_rmse[0]) - val_rmse_best if val_rmse else np.nan\n",
    "\n",
    "    preds = booster.predict(dval, iteration_range=(0, trained_rounds))\n",
    "    pred_s = pd.Series(preds, index=X_val.index)\n",
    "    y_val_s = y_val.reindex(X_val.index)\n",
    "\n",
    "    ic_by_date = []\n",
    "    for d, grp in pred_s.groupby(level=0):\n",
    "        y_grp = y_val_s.loc[(d, slice(None))]\n",
    "        x = grp.droplevel(0)\n",
    "        y = y_grp.droplevel(0)\n",
    "        ic_by_date.append(_rank_corr(x, y))\n",
    "    val_ic = float(np.nanmean(ic_by_date)) if ic_by_date else np.nan\n",
    "\n",
    "    return {\n",
    "        \"booster\": booster,\n",
    "        \"best_iteration\": int(best_iteration),\n",
    "        \"trained_rounds\": int(trained_rounds),\n",
    "        \"val_rmse_best\": val_rmse_best,\n",
    "        \"val_ic\": val_ic,\n",
    "        \"rmse_improve\": rmse_improve,\n",
    "    }\n",
    "\n",
    "\n",
    "def filter_universe(Xsig, pred, ysig, min_price, min_dvol):\n",
    "    if not isinstance(Xsig, pd.DataFrame):\n",
    "        raise ValueError(\"Xsig must be a DataFrame.\")\n",
    "    required = {\"px\", \"dvol_20\"}\n",
    "    missing = required - set(Xsig.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Xsig is missing columns: {sorted(missing)}\")\n",
    "\n",
    "    mask = (Xsig[\"px\"] >= min_price) & (Xsig[\"dvol_20\"] >= min_dvol)\n",
    "    tickers = Xsig.index[mask]\n",
    "\n",
    "    pred_u = pred.reindex(tickers)\n",
    "    y_u = ysig.reindex(tickers)\n",
    "\n",
    "    universe_meta = {\"n_universe\": int(len(tickers))}\n",
    "    return pred_u, y_u, universe_meta\n",
    "\n",
    "\n",
    "def select_portfolio(pred_u, n_long, n_short):\n",
    "    df = pred_u.dropna().reset_index()\n",
    "    df.columns = [\"ticker\", \"pred\"]\n",
    "    df = df.sort_values([\"pred\", \"ticker\"], ascending=[True, True], kind=\"mergesort\")\n",
    "\n",
    "    shorts = df.head(n_short)[\"ticker\"].tolist() if n_short > 0 else []\n",
    "    longs = df.tail(n_long)[\"ticker\"].tolist() if n_long > 0 else []\n",
    "    return longs, shorts\n",
    "\n",
    "\n",
    "def compute_weights(longs, shorts):\n",
    "    weights = {}\n",
    "    if longs:\n",
    "        w_long = 1.0 / len(longs)\n",
    "        weights.update({t: w_long for t in longs})\n",
    "    if shorts:\n",
    "        w_short = -1.0 / len(shorts)\n",
    "        weights.update({t: w_short for t in shorts})\n",
    "    return pd.Series(weights, name=\"weight\")\n",
    "\n",
    "\n",
    "def compute_turnover(prev_w, w):\n",
    "    if prev_w is None or prev_w.empty:\n",
    "        return 0.5 * float(w.abs().sum())\n",
    "    w_all = prev_w.reindex(prev_w.index.union(w.index)).fillna(0.0)\n",
    "    w_new = w.reindex(w_all.index).fillna(0.0)\n",
    "    return 0.5 * float((w_new - w_all).abs().sum())\n",
    "\n",
    "\n",
    "def run_backtest(fold_datasets, models, feat_cols, config):\n",
    "    perf_rows = []\n",
    "    weights_rows = []\n",
    "    holdings_rows = []\n",
    "    universe_rows = []\n",
    "\n",
    "    for fd in fold_datasets:\n",
    "        k = fd[\"fold\"]\n",
    "        model = models[k][\"booster\"]\n",
    "        X_test = fd[\"X_test\"].sort_index()\n",
    "        y_test = fd[\"y_test\"].sort_index()\n",
    "\n",
    "        prev_w = None\n",
    "        for d in X_test.index.get_level_values(0).unique():\n",
    "            Xsig = X_test.xs(d, level=0).sort_index()\n",
    "            ysig = y_test.xs(d, level=0).sort_index()\n",
    "\n",
    "            pred = pd.Series(\n",
    "                model.predict(xgb.DMatrix(Xsig[feat_cols])),\n",
    "                index=Xsig.index,\n",
    "                name=\"pred\",\n",
    "            )\n",
    "\n",
    "            pred_u, y_u, meta = filter_universe(\n",
    "                Xsig,\n",
    "                pred,\n",
    "                ysig,\n",
    "                config[\"MIN_PRICE\"],\n",
    "                config[\"MIN_DVOL_20\"],\n",
    "            )\n",
    "\n",
    "            if config.get(\"USE_RET_CLIP\", False):\n",
    "                y_u = y_u.clip(config[\"RET_CLIP_LOWER\"], config[\"RET_CLIP_UPPER\"])\n",
    "\n",
    "            longs, shorts = select_portfolio(pred_u, config[\"N_LONG\"], config[\"N_SHORT\"])\n",
    "            w = compute_weights(longs, shorts)\n",
    "\n",
    "            turnover = compute_turnover(prev_w, w)\n",
    "            cost = turnover * (config[\"COST_BPS_ONE_WAY\"] / 10_000)\n",
    "            gross_ret = float((w * y_u.reindex(w.index)).sum()) if not w.empty else 0.0\n",
    "            net_ret = gross_ret - cost\n",
    "\n",
    "            perf_rows.append(\n",
    "                {\n",
    "                    \"fold\": k,\n",
    "                    \"signal_date\": pd.Timestamp(d),\n",
    "                    \"gross_ret\": gross_ret,\n",
    "                    \"net_ret\": net_ret,\n",
    "                    \"turnover\": turnover,\n",
    "                    \"n_universe\": meta[\"n_universe\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            for t, wt in w.items():\n",
    "                holdings_rows.append(\n",
    "                    {\n",
    "                        \"fold\": k,\n",
    "                        \"signal_date\": pd.Timestamp(d),\n",
    "                        \"ticker\": t,\n",
    "                        \"side\": \"long\" if wt > 0 else \"short\",\n",
    "                        \"weight\": wt,\n",
    "                        \"pred\": pred_u.get(t, np.nan),\n",
    "                        \"asset_return\": y_u.get(t, np.nan),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            for t, pr in pred_u.items():\n",
    "                universe_rows.append(\n",
    "                    {\n",
    "                        \"fold\": k,\n",
    "                        \"signal_date\": pd.Timestamp(d),\n",
    "                        \"ticker\": t,\n",
    "                        \"pred\": pr,\n",
    "                        \"asset_return\": y_u.get(t, np.nan),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            for t, wt in w.items():\n",
    "                weights_rows.append(\n",
    "                    {\n",
    "                        \"fold\": k,\n",
    "                        \"signal_date\": pd.Timestamp(d),\n",
    "                        \"ticker\": t,\n",
    "                        \"weight\": wt,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            prev_w = w\n",
    "\n",
    "    perf = pd.DataFrame(perf_rows)\n",
    "    weights = pd.DataFrame(weights_rows)\n",
    "    holdings = pd.DataFrame(holdings_rows)\n",
    "    universe_panel = pd.DataFrame(universe_rows)\n",
    "\n",
    "    return {\n",
    "        \"perf\": perf,\n",
    "        \"weights\": weights,\n",
    "        \"holdings\": holdings,\n",
    "        \"universe_panel\": universe_panel,\n",
    "    }\n",
    "\n",
    "\n",
    "def dedupe_live(perf, weights, holdings):\n",
    "    perf = perf.sort_values([\"signal_date\", \"fold\"])\n",
    "    latest = perf.groupby(\"signal_date\")[\"fold\"].idxmax()\n",
    "    perf_live = perf.loc[latest].reset_index(drop=True)\n",
    "\n",
    "    keep = perf_live[[\"signal_date\", \"fold\"]]\n",
    "    weights_live = weights.merge(keep, on=[\"signal_date\", \"fold\"], how=\"inner\")\n",
    "    holdings_live = holdings.merge(keep, on=[\"signal_date\", \"fold\"], how=\"inner\")\n",
    "\n",
    "    return {\n",
    "        \"perf_live\": perf_live,\n",
    "        \"weights_live\": weights_live,\n",
    "        \"holdings_live\": holdings_live,\n",
    "    }\n",
    "\n",
    "\n",
    "def ic_timeseries(panel):\n",
    "    rows = []\n",
    "    for keys, grp in panel.groupby([\"signal_date\", \"fold\"]):\n",
    "        d, f = keys\n",
    "        grp = grp.dropna(subset=[\"pred\", \"asset_return\"])\n",
    "        if len(grp) < 2:\n",
    "            continue\n",
    "        ic = _rank_corr(grp[\"pred\"], grp[\"asset_return\"])\n",
    "        rows.append({\"signal_date\": d, \"fold\": f, \"ic\": ic})\n",
    "\n",
    "    ic_df = pd.DataFrame(rows)\n",
    "    if not ic_df.empty:\n",
    "        mean_ic = ic_df[\"ic\"].mean()\n",
    "        t_stat = mean_ic / ic_df[\"ic\"].std(ddof=1) * np.sqrt(len(ic_df)) if len(ic_df) > 1 else np.nan\n",
    "        ic_df.attrs.update({\"mean_ic\": mean_ic, \"t_stat\": t_stat})\n",
    "    return ic_df\n",
    "\n",
    "\n",
    "def decile_curve(universe_panel):\n",
    "    rows = []\n",
    "    for keys, grp in universe_panel.groupby([\"signal_date\", \"fold\"]):\n",
    "        d, f = keys\n",
    "        grp = grp.dropna(subset=[\"pred\", \"asset_return\"])\n",
    "        if len(grp) < 10:\n",
    "            continue\n",
    "        grp = grp.copy()\n",
    "        grp[\"decile\"] = pd.qcut(grp[\"pred\"], 10, labels=False, duplicates=\"drop\")\n",
    "        deci = grp.groupby(\"decile\")[\"asset_return\"].mean()\n",
    "        for dec, val in deci.items():\n",
    "            rows.append({\"signal_date\": d, \"fold\": f, \"decile\": int(dec), \"ret\": float(val)})\n",
    "\n",
    "    decile_df = pd.DataFrame(rows)\n",
    "    summary = None\n",
    "    if not decile_df.empty:\n",
    "        summary = decile_df.groupby(\"decile\")[\"ret\"].mean().rename(\"mean_ret\")\n",
    "        top_bottom = summary.loc[summary.index.max()] - summary.loc[summary.index.min()]\n",
    "        summary = summary.to_frame()\n",
    "        summary.loc[\"top_bottom\", \"mean_ret\"] = top_bottom\n",
    "    return decile_df, summary\n",
    "\n",
    "\n",
    "def ls_spread_timeseries(universe_panel):\n",
    "    rows = []\n",
    "    for keys, grp in universe_panel.groupby([\"signal_date\", \"fold\"]):\n",
    "        d, f = keys\n",
    "        grp = grp.dropna(subset=[\"pred\", \"asset_return\"])\n",
    "        if len(grp) < 10:\n",
    "            continue\n",
    "        grp = grp.copy()\n",
    "        grp[\"decile\"] = pd.qcut(grp[\"pred\"], 10, labels=False, duplicates=\"drop\")\n",
    "        top = grp[grp[\"decile\"] == grp[\"decile\"].max()][\"asset_return\"].mean()\n",
    "        bot = grp[grp[\"decile\"] == grp[\"decile\"].min()][\"asset_return\"].mean()\n",
    "        rows.append({\"signal_date\": d, \"fold\": f, \"ls_spread\": top - bot})\n",
    "\n",
    "    ls_df = pd.DataFrame(rows)\n",
    "    if not ls_df.empty:\n",
    "        mean_ls = ls_df[\"ls_spread\"].mean()\n",
    "        t_stat = mean_ls / ls_df[\"ls_spread\"].std(ddof=1) * np.sqrt(len(ls_df)) if len(ls_df) > 1 else np.nan\n",
    "        ls_df.attrs.update({\"mean_ls\": mean_ls, \"t_stat\": t_stat})\n",
    "    return ls_df\n",
    "\n",
    "\n",
    "def turnover_report(perf_live):\n",
    "    df = perf_live.dropna(subset=[\"turnover\", \"net_ret\"]).copy()\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    df[\"turnover_q\"] = pd.qcut(df[\"turnover\"], 4, labels=False, duplicates=\"drop\")\n",
    "    report = df.groupby(\"turnover_q\").agg(\n",
    "        mean_turnover=(\"turnover\", \"mean\"),\n",
    "        mean_net_ret=(\"net_ret\", \"mean\"),\n",
    "        count=(\"net_ret\", \"count\"),\n",
    "    )\n",
    "    return report\n",
    "\n",
    "\n",
    "def cost_sensitivity(perf_inputs, cost_grid=(0, 5, 10, 20, 50)):\n",
    "    if \"gross_ret\" not in perf_inputs or \"turnover\" not in perf_inputs:\n",
    "        raise ValueError(\"perf_inputs must include gross_ret and turnover columns.\")\n",
    "    rows = []\n",
    "    for cost in cost_grid:\n",
    "        net = perf_inputs[\"gross_ret\"] - perf_inputs[\"turnover\"] * (cost / 10_000)\n",
    "        rows.append({\"cost_bps\": cost, \"mean_net_ret\": net.mean()})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def export_quarter_holdings_excel(holdings_live, date, path, n_long, n_short):\n",
    "    d = pd.Timestamp(date)\n",
    "    df = holdings_live[holdings_live[\"signal_date\"] == d]\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No holdings found for the requested date.\")\n",
    "    longs = df[df[\"side\"] == \"long\"].nlargest(n_long, \"weight\")\n",
    "    shorts = df[df[\"side\"] == \"short\"].nsmallest(n_short, \"weight\")\n",
    "\n",
    "    with pd.ExcelWriter(path) as writer:\n",
    "        longs[[\"ticker\", \"weight\", \"asset_return\"]].to_excel(writer, sheet_name=\"LONG\", index=False)\n",
    "        shorts[[\"ticker\", \"weight\", \"asset_return\"]].to_excel(writer, sheet_name=\"SHORT\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9703d25b-775c-42b0-8bc6-f0c8bad4851e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "px_stocks: (21474668, 5) 2005-01-03 00:00:00 2025-12-19 00:00:00 9306\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "while PROJECT_ROOT.name != \"Bison\" and PROJECT_ROOT.parent != PROJECT_ROOT:\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "SEP_CACHE_DIR = (PROJECT_ROOT / \"cache_sharadar\" / \"sep_by_year\").resolve()\n",
    "\n",
    "def _load_year(y):\n",
    "    p = SEP_CACHE_DIR / f\"sep_{y}.parquet\"\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    df = pd.read_parquet(p)\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"ticker\", \"date\"])\n",
    "    df[\"ticker\"] = df[\"ticker\"].astype(str)\n",
    "    return df\n",
    "\n",
    "def load_sep_from_year_cache(start, end):\n",
    "    start = pd.Timestamp(start)\n",
    "    end = pd.Timestamp(end)\n",
    "    years = range(start.year, end.year + 1)\n",
    "\n",
    "    dfs = []\n",
    "    missing = []\n",
    "    for y in years:\n",
    "        df = _load_year(y)\n",
    "        if df is None:\n",
    "            missing.append(y)\n",
    "            continue\n",
    "        df = df[(df[\"date\"] >= start) & (df[\"date\"] <= end)]\n",
    "        if not df.empty:\n",
    "            dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        raise ValueError(f\"No SEP data loaded. Missing years: {missing}. Dir: {SEP_CACHE_DIR}\")\n",
    "\n",
    "    px = pd.concat(dfs, ignore_index=True)\n",
    "    px = px.drop_duplicates(subset=[\"ticker\", \"date\"], keep=\"last\")\n",
    "    px = px.sort_values([\"date\", \"ticker\"]).reset_index(drop=True)\n",
    "    return px\n",
    "\n",
    "px_stocks = load_sep_from_year_cache(START, END)\n",
    "print(\"px_stocks:\", px_stocks.shape, px_stocks[\"date\"].min(), px_stocks[\"date\"].max(), px_stocks[\"ticker\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1216315f-6630-4103-8b05-a48e1ecdce45",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trading days: 5276\n",
      "Months in backtest: 250\n",
      "Signal range: 2005-01-31 -> 2025-10-31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([(Timestamp('2005-01-31 00:00:00'),\n",
       "   Timestamp('2005-02-01 00:00:00'),\n",
       "   Timestamp('2005-03-01 00:00:00')),\n",
       "  (Timestamp('2005-02-28 00:00:00'),\n",
       "   Timestamp('2005-03-01 00:00:00'),\n",
       "   Timestamp('2005-04-01 00:00:00')),\n",
       "  (Timestamp('2005-03-31 00:00:00'),\n",
       "   Timestamp('2005-04-01 00:00:00'),\n",
       "   Timestamp('2005-05-02 00:00:00'))],\n",
       " [(Timestamp('2025-08-29 00:00:00'),\n",
       "   Timestamp('2025-09-02 00:00:00'),\n",
       "   Timestamp('2025-10-01 00:00:00')),\n",
       "  (Timestamp('2025-09-30 00:00:00'),\n",
       "   Timestamp('2025-10-01 00:00:00'),\n",
       "   Timestamp('2025-11-03 00:00:00')),\n",
       "  (Timestamp('2025-10-31 00:00:00'),\n",
       "   Timestamp('2025-11-03 00:00:00'),\n",
       "   Timestamp('2025-12-01 00:00:00'))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build trading calendar\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _month_end_trading_days(trading_days_index):\n",
    "    s = pd.Series(trading_days_index, index=trading_days_index)\n",
    "    return s.groupby(s.index.to_period(\"M\")).max().sort_values().tolist()\n",
    "\n",
    "def _next_trading_day(trading_days_index, d):\n",
    "    d = pd.Timestamp(d)\n",
    "    pos = trading_days_index.searchsorted(d)\n",
    "    if pos >= len(trading_days_index) - 1:\n",
    "        return None\n",
    "    if trading_days_index[pos] == d:\n",
    "        return trading_days_index[pos + 1]\n",
    "    return trading_days_index[pos]\n",
    "\n",
    "def _pick_price_col(df):\n",
    "    if \"closeadj\" in df.columns:\n",
    "        return \"closeadj\"\n",
    "    if \"close\" in df.columns:\n",
    "        return \"close\"\n",
    "    raise ValueError(\"No close/closeadj column found in SEP data.\")\n",
    "\n",
    "# px_stocks is your loaded SEP dataframe\n",
    "px_stocks[\"date\"] = pd.to_datetime(px_stocks[\"date\"])\n",
    "price_col = _pick_price_col(px_stocks)\n",
    "\n",
    "stock_close = px_stocks.pivot(index=\"date\", columns=\"ticker\", values=price_col).sort_index()\n",
    "stock_vol = None\n",
    "if \"volume\" in px_stocks.columns:\n",
    "    stock_vol = px_stocks.pivot(index=\"date\", columns=\"ticker\", values=\"volume\").sort_index()\n",
    "\n",
    "all_days = stock_close.index.sort_values()\n",
    "\n",
    "month_ends = _month_end_trading_days(all_days)\n",
    "month_ends = [d for d in month_ends if (pd.Timestamp(START) <= pd.Timestamp(d) <= pd.Timestamp(END))]\n",
    "\n",
    "pairs = []\n",
    "for i in range(len(month_ends) - 1):\n",
    "    sig = pd.Timestamp(month_ends[i])                 # signal at EOM close\n",
    "    entry = _next_trading_day(all_days, sig)          # trade next trading day close\n",
    "    next_sig = pd.Timestamp(month_ends[i + 1])        # next EOM close\n",
    "    exit_ = _next_trading_day(all_days, next_sig)     # exit next trading day close after next EOM\n",
    "    if entry is None or exit_ is None:\n",
    "        continue\n",
    "    pairs.append((sig, entry, exit_))\n",
    "\n",
    "if not pairs:\n",
    "    raise ValueError(\"No valid monthly entry/exit pairs found. Check START/END vs cached SEP dates.\")\n",
    "\n",
    "signal_dates = [p[0] for p in pairs]\n",
    "\n",
    "print(\"Trading days:\", len(all_days))\n",
    "print(\"Months in backtest:\", len(pairs))\n",
    "print(\"Signal range:\", signal_dates[0].date(), \"->\", signal_dates[-1].date())\n",
    "pairs[:3], pairs[-3:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c79e4524-5941-4a10-b559-a8eb1c51669d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eligible names per month:\n",
      "count     250.000000\n",
      "mean     1562.176000\n",
      "std       210.099367\n",
      "min      1021.000000\n",
      "25%      1413.500000\n",
      "50%      1594.500000\n",
      "75%      1699.750000\n",
      "max      2108.000000\n",
      "dtype: float64\n",
      "\n",
      "Months below 5th pct threshold ( 1201 ):\n",
      "signal_date\n",
      "2008-11-28    1101\n",
      "2008-12-31    1060\n",
      "2009-01-30    1048\n",
      "2009-02-27    1021\n",
      "2009-03-31    1036\n",
      "2009-04-30    1126\n",
      "2009-05-29    1181\n",
      "2009-06-30    1178\n",
      "2009-07-31    1164\n",
      "2009-08-31    1185\n",
      "2009-11-30    1200\n",
      "2009-12-31    1201\n",
      "2010-07-30    1201\n",
      "2010-08-31    1177\n",
      "dtype: int64\n",
      "\n",
      "Shapes:\n",
      "pairs_df: (250, 2)\n",
      "fwd_stock: (250, 9306)\n",
      "fwd_bench NaNs: 0\n",
      "y_excess rows: 2326500\n"
     ]
    }
   ],
   "source": [
    "# ---- inputs assumed already defined ----\n",
    "# stock_close (date x ticker), stock_vol (date x ticker or None)\n",
    "# pairs (list of (signal_date, entry_date, exit_date))\n",
    "# START, END, MIN_PRICE, MIN_DVOL_20\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs, columns=[\"signal_date\", \"trade_date\", \"exit_date\"]).set_index(\"signal_date\").sort_index()\n",
    "\n",
    "# forward returns for every ticker for each month (entry close -> exit close)\n",
    "trade_px = stock_close.reindex(pairs_df[\"trade_date\"].values)\n",
    "exit_px  = stock_close.reindex(pairs_df[\"exit_date\"].values)\n",
    "\n",
    "trade_px.index = pairs_df.index\n",
    "exit_px.index  = pairs_df.index\n",
    "\n",
    "fwd_stock = (exit_px / trade_px - 1.0).astype(\"float32\")\n",
    "fwd_stock.index.name = \"signal_date\"\n",
    "\n",
    "# ---- strict eligibility at signal date (EOM close) ----\n",
    "sig_px = stock_close.reindex(pairs_df.index)\n",
    "\n",
    "eligible = (sig_px >= MIN_PRICE)\n",
    "\n",
    "if stock_vol is not None:\n",
    "    dvol_20_daily = (stock_vol * stock_close).rolling(20, min_periods=20).mean()\n",
    "    dvol_20_sig = dvol_20_daily.reindex(pairs_df.index)\n",
    "    eligible = eligible & (dvol_20_sig >= MIN_DVOL_20)\n",
    "\n",
    "# extra strict: require valid trade/exit prices too (prevents weird division artifacts)\n",
    "eligible = eligible & trade_px.notna() & exit_px.notna()\n",
    "\n",
    "# diagnostics (you already liked these)\n",
    "elig_counts = eligible.sum(axis=1).astype(int)\n",
    "print(\"Eligible names per month:\")\n",
    "print(elig_counts.describe())\n",
    "\n",
    "low_cut = int(elig_counts.quantile(0.05))\n",
    "print(\"\\nMonths below 5th pct threshold (\", low_cut, \"):\")\n",
    "print(elig_counts[elig_counts <= low_cut].head(30))\n",
    "\n",
    "# ---- synthetic benchmark = average eligible forward return each month ----\n",
    "fwd_bench = fwd_stock.where(eligible).mean(axis=1).astype(\"float32\")\n",
    "fwd_bench.name = \"bench_fwd\"\n",
    "\n",
    "# ---- excess forward return target y(date,ticker) ----\n",
    "y_excess = fwd_stock.sub(fwd_bench, axis=0).stack(future_stack=True).astype(\"float32\")\n",
    "y_excess.index.set_names([\"date\", \"ticker\"], inplace=True)\n",
    "y_excess.name = \"y\"\n",
    "\n",
    "print(\"\\nShapes:\")\n",
    "print(\"pairs_df:\", pairs_df.shape)\n",
    "print(\"fwd_stock:\", fwd_stock.shape)\n",
    "print(\"fwd_bench NaNs:\", int(fwd_bench.isna().sum()))\n",
    "print(\"y_excess rows:\", int(len(y_excess)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67dda621-da33-4416-a926-f7e5f1647a31",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cached X_price: (2326500, 8)\n",
      "X_price: (2326500, 8)\n",
      "NA rate (top 12):\n",
      "ret_252    0.608837\n",
      "vol_252    0.608837\n",
      "ret_126    0.585915\n",
      "ret_63     0.574187\n",
      "vol_63     0.574187\n",
      "ret_21     0.566300\n",
      "dvol_20    0.563903\n",
      "px         0.562341\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# build price and vol features\n",
    "# -----------------------------\n",
    "# Build PRICE features at signal dates (and cache to parquet)\n",
    "# -----------------------------\n",
    "\n",
    "cache_path = Path(CACHE_DIR) / f\"X_price_{pd.Timestamp(START).date()}_{pd.Timestamp(END).date()}.parquet\"\n",
    "\n",
    "if cache_path.exists():\n",
    "    X_price = pd.read_parquet(cache_path)\n",
    "    X_price.index = pd.MultiIndex.from_frame(\n",
    "        X_price.index.to_frame(index=False).rename(columns={0: \"date\", 1: \"ticker\"})\n",
    "    ) if not isinstance(X_price.index, pd.MultiIndex) else X_price.index\n",
    "    X_price.index.set_names([\"date\", \"ticker\"], inplace=True)\n",
    "    print(\"Loaded cached X_price:\", X_price.shape)\n",
    "else:\n",
    "    stock_close = stock_close.sort_index()\n",
    "    if stock_vol is not None:\n",
    "        stock_vol = stock_vol.reindex(stock_close.index).sort_index()\n",
    "\n",
    "    sig = pd.Index(pd.to_datetime(signal_dates)).unique().sort_values()\n",
    "    sig = sig.intersection(stock_close.index)\n",
    "\n",
    "    print(\"signal months:\", len(sig))\n",
    "    print(\"tickers:\", stock_close.shape[1])\n",
    "    print(\"price range:\", stock_close.index.min(), \"->\", stock_close.index.max())\n",
    "\n",
    "    daily_ret = stock_close.pct_change(fill_method=None)\n",
    "\n",
    "    ret_21  = stock_close / stock_close.shift(21)  - 1.0\n",
    "    ret_63  = stock_close / stock_close.shift(63)  - 1.0\n",
    "    ret_126 = stock_close / stock_close.shift(126) - 1.0\n",
    "    ret_252 = stock_close / stock_close.shift(252) - 1.0\n",
    "\n",
    "    vol_63  = daily_ret.rolling(63).std()\n",
    "    vol_252 = daily_ret.rolling(252).std()\n",
    "\n",
    "    dvol_20 = None\n",
    "    if stock_vol is not None:\n",
    "        dvol_20 = (stock_vol * stock_close).rolling(20).mean()\n",
    "\n",
    "    def _stack_at(mat, name):\n",
    "        tmp = mat.loc[sig].stack(future_stack=True).rename(name).to_frame()\n",
    "        tmp.index.set_names([\"date\", \"ticker\"], inplace=True)\n",
    "        return tmp\n",
    "\n",
    "    X_parts = [\n",
    "        _stack_at(ret_21,  \"ret_21\"),\n",
    "        _stack_at(ret_63,  \"ret_63\"),\n",
    "        _stack_at(ret_126, \"ret_126\"),\n",
    "        _stack_at(ret_252, \"ret_252\"),\n",
    "        _stack_at(vol_63,  \"vol_63\"),\n",
    "        _stack_at(vol_252, \"vol_252\"),\n",
    "    ]\n",
    "\n",
    "    if dvol_20 is not None:\n",
    "        X_parts.append(_stack_at(dvol_20, \"dvol_20\"))\n",
    "\n",
    "    px_sig = stock_close.loc[sig].stack(future_stack=True).rename(\"px\").to_frame()\n",
    "    px_sig.index.set_names([\"date\", \"ticker\"], inplace=True)\n",
    "    X_parts.append(px_sig)\n",
    "\n",
    "    X_price = pd.concat(X_parts, axis=1).sort_index()\n",
    "    X_price = X_price[~X_price.index.duplicated(keep=\"last\")]\n",
    "\n",
    "    for c in X_price.columns:\n",
    "        X_price[c] = X_price[c].astype(\"float32\", copy=False)\n",
    "\n",
    "    X_price.to_parquet(cache_path)\n",
    "    print(\"Saved X_price:\", cache_path)\n",
    "\n",
    "print(\"X_price:\", X_price.shape)\n",
    "print(\"NA rate (top 12):\")\n",
    "print(X_price.isna().mean().sort_values(ascending=False).head(12))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c317bc72-59dd-463a-83e9-b334de95a310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped all-NA factors: ['roe']\n",
      "Lookahead violations: 0\n",
      "SF1 aligned saved: (2326500, 21) to cache_sharadar\\sf1_factors_aligned_ARQ_2005-01-01_2025-12-31.parquet\n",
      "X_full: (2326500, 29)\n",
      "NA rate (top 15):\n",
      "val_pe              0.574149\n",
      "inv_growth_yoy      0.571228\n",
      "val_evebitda        0.483093\n",
      "wc_chg_assets       0.381220\n",
      "currentratio        0.358379\n",
      "debt_growth_yoy     0.339700\n",
      "val_pb              0.314150\n",
      "val_ps              0.281596\n",
      "rev_yoy             0.272307\n",
      "eps_yoy             0.257342\n",
      "ebitdamargin        0.254324\n",
      "netmargin           0.252348\n",
      "grossmargin         0.252331\n",
      "asset_growth_yoy    0.237264\n",
      "divyield            0.232132\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "CACHE_DIR = Path(CACHE_DIR)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DIM = \"ARQ\"\n",
    "SF1_RAW_PATH = CACHE_DIR / f\"sf1_raw_{DIM}_{pd.Timestamp(START).date()}_{pd.Timestamp(END).date()}.parquet\"\n",
    "SF1_ALIGNED_PATH = CACHE_DIR / f\"sf1_factors_aligned_{DIM}_{pd.Timestamp(START).date()}_{pd.Timestamp(END).date()}.parquet\"\n",
    "\n",
    "INDICATORS_CSV_PATH = Path(\"SF1 Indicators.csv\")\n",
    "\n",
    "ind_meta = pd.read_csv(INDICATORS_CSV_PATH)\n",
    "sf1_indicators = set(ind_meta.loc[ind_meta[\"table\"].eq(\"SF1\"), \"indicator\"].astype(str))\n",
    "\n",
    "def _to_num(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def _safe_div(a, b):\n",
    "    a = _to_num(a)\n",
    "    b = _to_num(b)\n",
    "    return a / b.replace(0, np.nan)\n",
    "\n",
    "def _safe_log_pos(x):\n",
    "    x = _to_num(x)\n",
    "    x = x.where(x > 0)\n",
    "    return np.log(x)\n",
    "\n",
    "def _neglog_pos(x):\n",
    "    return -_safe_log_pos(x)\n",
    "\n",
    "def _pct_change_4q(s):\n",
    "    s = _to_num(s)\n",
    "    return s.groupby(level=0).pct_change(4, fill_method=None)\n",
    "\n",
    "def _diff_4q(s):\n",
    "    s = _to_num(s)\n",
    "    return s.groupby(level=0).diff(4)\n",
    "\n",
    "def _shift_avail(d):\n",
    "    return _shift_trading_days(all_days, d, FUNDAMENTALS_LAG_TRADING_DAYS)\n",
    "\n",
    "def _col(name):\n",
    "    return name if name in sf1_indicators else None\n",
    "\n",
    "def _nonempty(s):\n",
    "    s = pd.Series(s)\n",
    "    return s.notna().any()\n",
    "\n",
    "if SF1_RAW_PATH.exists():\n",
    "    sf1_raw = pd.read_parquet(SF1_RAW_PATH)\n",
    "else:\n",
    "    sf1_start = (pd.Timestamp(START) - pd.Timedelta(days=365 * 2)).strftime(\"%Y-%m-%d\")\n",
    "    tickers_used = sorted(stock_close.columns.astype(str).tolist())\n",
    "    sf1_raw = fetch_sf1_fundamentals_all_numeric(\n",
    "        tickers=tickers_used,\n",
    "        start=sf1_start,\n",
    "        end=END,\n",
    "        dimension=DIM,\n",
    "        cache_name=f\"sf1_raw_{DIM}_{pd.Timestamp(START).date()}_{pd.Timestamp(END).date()}\",\n",
    "    )\n",
    "    sf1_raw.to_parquet(SF1_RAW_PATH)\n",
    "\n",
    "sf1 = sf1_raw.copy()\n",
    "sf1[\"ticker\"] = sf1[\"ticker\"].astype(str)\n",
    "sf1[\"calendardate\"] = pd.to_datetime(sf1.get(\"calendardate\", pd.NaT), errors=\"coerce\")\n",
    "sf1[\"datekey\"] = pd.to_datetime(sf1.get(\"datekey\", pd.NaT), errors=\"coerce\")\n",
    "sf1 = sf1.dropna(subset=[\"ticker\", \"calendardate\", \"datekey\"]).sort_values([\"ticker\", \"calendardate\"], kind=\"mergesort\")\n",
    "\n",
    "assets_c = _col(\"assets\")\n",
    "assetsavg_c = _col(\"assetsavg\")\n",
    "assetsc_c = _col(\"assetsc\")\n",
    "liabilitiesc_c = _col(\"liabilitiesc\")\n",
    "liabilities_c = _col(\"liabilities\")\n",
    "debt_c = _col(\"debt\")\n",
    "revenue_c = _col(\"revenue\")\n",
    "revenueusd_c = _col(\"revenueusd\")\n",
    "netinc_c = _col(\"netinc\")\n",
    "ncfo_c = _col(\"ncfo\")\n",
    "capex_c = _col(\"capex\")\n",
    "inventory_c = _col(\"inventory\")\n",
    "mcap_c = _col(\"marketcap\")\n",
    "epsdil_c = _col(\"epsdil\")\n",
    "eps_c = _col(\"eps\")\n",
    "epsusd_c = _col(\"epsusd\")\n",
    "divyield_c = _col(\"divyield\")\n",
    "pe_c = _col(\"pe\")\n",
    "pb_c = _col(\"pb\")\n",
    "ps_c = _col(\"ps\")\n",
    "evebitda_c = _col(\"evebitda\")\n",
    "grossmargin_c = _col(\"grossmargin\")\n",
    "ebitdamargin_c = _col(\"ebitdamargin\")\n",
    "netmargin_c = _col(\"netmargin\")\n",
    "currentratio_c = _col(\"currentratio\")\n",
    "assetturnover_c = _col(\"assetturnover\")\n",
    "roa_c = _col(\"roa\")\n",
    "roe_c = _col(\"roe\")\n",
    "\n",
    "sf1_keyed = sf1.set_index([\"ticker\", \"calendardate\"], drop=False)\n",
    "\n",
    "assets_base = None\n",
    "if assets_c and assets_c in sf1_keyed.columns:\n",
    "    assets_base = sf1_keyed[assets_c]\n",
    "elif assetsavg_c and assetsavg_c in sf1_keyed.columns:\n",
    "    assets_base = sf1_keyed[assetsavg_c]\n",
    "\n",
    "revenue_base = None\n",
    "if revenue_c and revenue_c in sf1_keyed.columns:\n",
    "    revenue_base = sf1_keyed[revenue_c]\n",
    "elif revenueusd_c and revenueusd_c in sf1_keyed.columns:\n",
    "    revenue_base = sf1_keyed[revenueusd_c]\n",
    "\n",
    "eps_base = None\n",
    "for nm in [epsdil_c, eps_c, epsusd_c]:\n",
    "    if nm and nm in sf1_keyed.columns:\n",
    "        eps_base = sf1_keyed[nm]\n",
    "        break\n",
    "\n",
    "val_pe = _neglog_pos(sf1_keyed[pe_c]) if (pe_c and pe_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "val_pb = _neglog_pos(sf1_keyed[pb_c]) if (pb_c and pb_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "val_ps = _neglog_pos(sf1_keyed[ps_c]) if (ps_c and ps_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "val_evebitda = _neglog_pos(sf1_keyed[evebitda_c]) if (evebitda_c and evebitda_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "\n",
    "log_mcap = _safe_log_pos(sf1_keyed[mcap_c]) if (mcap_c and mcap_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "\n",
    "grossmargin = _to_num(sf1_keyed[grossmargin_c]) if (grossmargin_c and grossmargin_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "ebitdamargin = _to_num(sf1_keyed[ebitdamargin_c]) if (ebitdamargin_c and ebitdamargin_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "netmargin = _to_num(sf1_keyed[netmargin_c]) if (netmargin_c and netmargin_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "\n",
    "roa = _to_num(sf1_keyed[roa_c]) if (roa_c and roa_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "roe = _to_num(sf1_keyed[roe_c]) if (roe_c and roe_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "assetturnover = _to_num(sf1_keyed[assetturnover_c]) if (assetturnover_c and assetturnover_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "currentratio = _to_num(sf1_keyed[currentratio_c]) if (currentratio_c and currentratio_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "\n",
    "rev_yoy = _pct_change_4q(revenue_base) if revenue_base is not None else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "eps_yoy = _pct_change_4q(eps_base) if eps_base is not None else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "asset_growth_yoy = _pct_change_4q(assets_base) if assets_base is not None else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "\n",
    "capex_assets = (\n",
    "    _safe_div(sf1_keyed[capex_c], assets_base)\n",
    "    if (capex_c and capex_c in sf1_keyed.columns_toggle if False else True)\n",
    "    else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    ")\n",
    "# rebuild capex_assets correctly (avoid any accidental NameError)\n",
    "if capex_c and capex_c in sf1_keyed.columns and assets_base is not None:\n",
    "    capex_assets = _safe_div(sf1_keyed[capex_c], assets_base)\n",
    "else:\n",
    "    capex_assets = pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "\n",
    "inv_growth_yoy = _pct_change_4q(sf1_keyed[inventory_c]) if (inventory_c and inventory_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "debt_growth_yoy = _pct_change_4q(sf1_keyed[debt_c]) if (debt_c and debt_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "lev_debt_assets = _safe_div(sf1_keyed[debt_c], assets_base) if (debt_c and debt_c in sf1_keyed.columns and assets_base is not None) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "\n",
    "accruals_ni_ncfo_assets = (\n",
    "    _safe_div((_to_num(sf1_keyed[netinc_c]) - _to_num(sf1_keyed[ncfo_c])), assets_base)\n",
    "    if (netinc_c and netinc_c in sf1_keyed.columns and ncfo_c and ncfo_c in sf1_keyed.columns and assets_base is not None)\n",
    "    else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    ")\n",
    "\n",
    "if assetsc_c and assetsc_c in sf1_keyed.columns and liabilitiesc_c and liabilitiesc_c in sf1_keyed.columns and assets_base is not None:\n",
    "    wc = _to_num(sf1_keyed[assetsc_c]) - _to_num(sf1_keyed[liabilitiesc_c])\n",
    "    wc_chg_assets = _safe_div(_diff_4q(wc), assets_base)\n",
    "else:\n",
    "    wc_chg_assets = pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "\n",
    "divyield = _to_num(sf1_keyed[divyield_c]) if (divyield_c and divyield_c in sf1_keyed.columns) else pd.Series(index=sf1_keyed.index, dtype=\"float64\")\n",
    "\n",
    "if (not _nonempty(roa)) and (netinc_c and netinc_c in sf1_keyed.columns) and (assets_base is not None):\n",
    "    roa = _safe_div(sf1_keyed[netinc_c], assets_base)\n",
    "\n",
    "if (not _nonempty(assetturnover)) and (revenue_base is not None) and (assets_base is not None):\n",
    "    assetturnover = _safe_div(revenue_base, assets_base)\n",
    "\n",
    "sf1_f = pd.DataFrame(\n",
    "    {\n",
    "        \"ticker\": sf1_keyed[\"ticker\"].values,\n",
    "        \"datekey\": sf1_keyed[\"datekey\"].values,\n",
    "        \"calendardate\": sf1_keyed[\"calendardate\"].values,\n",
    "        \"val_pe\": val_pe.values,\n",
    "        \"val_pb\": val_pb.values,\n",
    "        \"val_ps\": val_ps.values,\n",
    "        \"val_evebitda\": val_evebitda.values,\n",
    "        \"log_mcap\": log_mcap.values,\n",
    "        \"grossmargin\": grossmargin.values,\n",
    "        \"ebitdamargin\": ebitdamargin.values,\n",
    "        \"netmargin\": netmargin.values,\n",
    "        \"roa\": roa.values,\n",
    "        \"roe\": roe.values,\n",
    "        \"currentratio\": currentratio.values,\n",
    "        \"assetturnover\": assetturnover.values,\n",
    "        \"rev_yoy\": rev_yoy.values,\n",
    "        \"eps_yoy\": eps_yoy.values,\n",
    "        \"asset_growth_yoy\": asset_growth_yoy.values,\n",
    "        \"capex_assets\": capex_assets.values,\n",
    "        \"inv_growth_yoy\": inv_growth_yoy.values,\n",
    "        \"debt_growth_yoy\": debt_growth_yoy.values,\n",
    "        \"lev_debt_assets\": lev_debt_assets.values,\n",
    "        \"accruals_ni_ncfo_assets\": accruals_ni_ncfo_assets.values,\n",
    "        \"wc_chg_assets\": wc_chg_assets.values,\n",
    "        \"divyield\": divyield.values,\n",
    "    }\n",
    ")\n",
    "\n",
    "factor_cols_all = [\n",
    "    \"val_pe\",\"val_pb\",\"val_ps\",\"val_evebitda\",\n",
    "    \"log_mcap\",\n",
    "    \"grossmargin\",\"ebitdamargin\",\"netmargin\",\"roa\",\"roe\",\"currentratio\",\"assetturnover\",\n",
    "    \"rev_yoy\",\"eps_yoy\",\"asset_growth_yoy\",\"capex_assets\",\"inv_growth_yoy\",\"debt_growth_yoy\",\"lev_debt_assets\",\n",
    "    \"accruals_ni_ncfo_assets\",\"wc_chg_assets\",\n",
    "    \"divyield\",\n",
    "]\n",
    "factor_cols = [c for c in factor_cols_all if c in sf1_f.columns and _nonempty(sf1_f[c])]\n",
    "print(\"Dropped all-NA factors:\", [c for c in factor_cols_all if c not in factor_cols])\n",
    "\n",
    "sf1_f[\"avail_date\"] = pd.to_datetime(sf1_f[\"datekey\"], errors=\"coerce\").apply(_shift_avail)\n",
    "sf1_f = sf1_f.dropna(subset=[\"ticker\", \"avail_date\"])\n",
    "sf1_f = sf1_f.sort_values([\"ticker\", \"avail_date\", \"datekey\"], kind=\"mergesort\")\n",
    "sf1_f = sf1_f.drop_duplicates(subset=[\"ticker\", \"avail_date\"], keep=\"last\")\n",
    "\n",
    "if \"X_price\" in globals() and isinstance(X_price, pd.DataFrame):\n",
    "    X = X_price.copy()\n",
    "elif \"X\" in globals() and isinstance(X, pd.DataFrame):\n",
    "    X = X.copy()\n",
    "else:\n",
    "    raise NameError(\"Need X_price (or X) defined before SF1 alignment.\")\n",
    "\n",
    "left = X.index.to_frame(index=False).copy()\n",
    "left[\"date\"] = pd.to_datetime(left[\"date\"], errors=\"coerce\")\n",
    "left[\"ticker\"] = left[\"ticker\"].astype(str)\n",
    "left = left.dropna(subset=[\"date\", \"ticker\"])\n",
    "left = left.drop_duplicates(subset=[\"date\", \"ticker\"], keep=\"last\")\n",
    "left = left.sort_values([\"date\", \"ticker\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "right = sf1_f[[\"ticker\", \"avail_date\"] + factor_cols].copy()\n",
    "right[\"ticker\"] = right[\"ticker\"].astype(str)\n",
    "right[\"avail_date\"] = pd.to_datetime(right[\"avail_date\"], errors=\"coerce\")\n",
    "right = right.dropna(subset=[\"ticker\", \"avail_date\"])\n",
    "right = right.sort_values([\"avail_date\", \"ticker\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "merged = pd.merge_asof(\n",
    "    left,\n",
    "    right,\n",
    "    left_on=\"date\",\n",
    "    right_on=\"avail_date\",\n",
    "    by=\"ticker\",\n",
    "    direction=\"backward\",\n",
    "    allow_exact_matches=True,\n",
    ")\n",
    "\n",
    "lookahead = int((merged[\"avail_date\"].notna() & (merged[\"avail_date\"] > merged[\"date\"])).sum())\n",
    "print(\"Lookahead violations:\", lookahead)\n",
    "\n",
    "X_factors = merged.set_index([\"date\", \"ticker\"])[factor_cols].sort_index()\n",
    "for c in X_factors.columns:\n",
    "    X_factors[c] = X_factors[c].astype(\"float32\", copy=False)\n",
    "\n",
    "X_factors.to_parquet(SF1_ALIGNED_PATH)\n",
    "print(\"SF1 aligned saved:\", X_factors.shape, \"to\", SF1_ALIGNED_PATH)\n",
    "\n",
    "X_full = X.join(X_factors, how=\"left\").sort_index()\n",
    "print(\"X_full:\", X_full.shape)\n",
    "\n",
    "na_rate = X_factors.isna().mean().sort_values(ascending=False)\n",
    "print(\"NA rate (top 15):\")\n",
    "print(na_rate.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c43befc-0acf-406f-b83d-9899107445fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean factor coverage (bottom 10):\n",
      "val_pe             0.425851\n",
      "inv_growth_yoy     0.428772\n",
      "val_evebitda       0.516907\n",
      "wc_chg_assets      0.618780\n",
      "currentratio       0.641621\n",
      "debt_growth_yoy    0.660300\n",
      "val_pb             0.685850\n",
      "val_ps             0.718404\n",
      "rev_yoy            0.727693\n",
      "eps_yoy            0.742658\n",
      "dtype: float64\n",
      "Dropping low-coverage factors: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "C:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\numpy\\_core\\_methods.py:51: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_panel: (2326500, 50)\n",
      "Example columns: ['ret_21', 'ret_63', 'ret_126', 'ret_252', 'vol_63', 'vol_252', 'dvol_20', 'px', 'val_pe', 'inv_growth_yoy', 'val_evebitda', 'wc_chg_assets', 'currentratio', 'debt_growth_yoy', 'val_pb']\n"
     ]
    }
   ],
   "source": [
    "# winsorize and make X panel\n",
    "if \"X_full\" not in globals() or not isinstance(X_full, pd.DataFrame):\n",
    "    raise NameError(\"X_full not found. Run the SF1 alignment cell first so X_full exists.\")\n",
    "\n",
    "factor_cols = [c for c in X_full.columns if c not in X_price.columns] if \"X_price\" in globals() else [c for c in X_full.columns if c not in X.columns]\n",
    "factor_cols = [c for c in factor_cols if c in X_full.columns]\n",
    "\n",
    "panel = X_full.copy()\n",
    "\n",
    "def _winsorize_cs(df, lo=0.01, hi=0.99):\n",
    "    qlo = df.groupby(level=0).quantile(lo)\n",
    "    qhi = df.groupby(level=0).quantile(hi)\n",
    "\n",
    "    out = df.copy()\n",
    "    for c in df.columns:\n",
    "        lo_s = qlo[c]\n",
    "        hi_s = qhi[c]\n",
    "        out[c] = out[c].clip(lower=lo_s, upper=hi_s)\n",
    "    return out\n",
    "\n",
    "def _zscore_cs(df):\n",
    "    mu = df.groupby(level=0).transform(\"mean\")\n",
    "    sd = df.groupby(level=0).transform(lambda x: x.std(ddof=0))\n",
    "    return (df - mu) / sd.replace(0, np.nan)\n",
    "\n",
    "def _add_missing_indicators(df):\n",
    "    miss = df.isna().astype(\"int8\")\n",
    "    miss.columns = [f\"{c}__missing\" for c in miss.columns]\n",
    "    return miss\n",
    "\n",
    "base_cols = [c for c in panel.columns if c not in factor_cols]\n",
    "Xf = panel[factor_cols].astype(\"float32\").copy()\n",
    "\n",
    "coverage = Xf.notna().groupby(level=0).mean()\n",
    "mean_coverage = coverage.mean().sort_values()\n",
    "print(\"Mean factor coverage (bottom 10):\")\n",
    "print(mean_coverage.head(10))\n",
    "\n",
    "min_daily_coverage = 0.05\n",
    "keep_factors = mean_coverage[mean_coverage >= min_daily_coverage].index.tolist()\n",
    "drop_factors = [c for c in factor_cols if c not in keep_factors]\n",
    "print(\"Dropping low-coverage factors:\", drop_factors)\n",
    "\n",
    "Xf = Xf[keep_factors]\n",
    "\n",
    "winsor_lo = 0.01\n",
    "winsor_hi = 0.99\n",
    "Xf_w = _winsorize_cs(Xf, lo=winsor_lo, hi=winsor_hi)\n",
    "\n",
    "Xf_z = _zscore_cs(Xf_w)\n",
    "\n",
    "miss_ind = _add_missing_indicators(Xf_z)\n",
    "\n",
    "X_panel = pd.concat([panel[base_cols], Xf_z, miss_ind], axis=1).sort_index()\n",
    "\n",
    "print(\"X_panel:\", X_panel.shape)\n",
    "print(\"Example columns:\", X_panel.columns[:15].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdee3f76-4cf9-4189-9b33-bd5dabb06148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_23624\\2434841225.py:25: FutureWarning: The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.\n",
      "  y = fwd_ret.stack(dropna=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y coverage: 0.7835529765742532 non-NA: 1822936 total: 2326500\n",
      "y sample: {(Timestamp('2005-01-31 00:00:00'), 'A'): -0.05156037991858886, (Timestamp('2005-01-31 00:00:00'), 'AABC'): -0.0034602076124566894, (Timestamp('2005-01-31 00:00:00'), 'AACC'): 0.02006707975669375, (Timestamp('2005-01-31 00:00:00'), 'AACE'): -0.10984848484848475, (Timestamp('2005-01-31 00:00:00'), 'AAI'): -0.03161592505854793}\n"
     ]
    }
   ],
   "source": [
    "# establish 3 month total return for backtest\n",
    "\n",
    "if \"px_stocks\" not in globals() or not isinstance(px_stocks, pd.DataFrame):\n",
    "    raise NameError(\"px_stocks not found. Load SEP first so px_stocks exists with columns ['ticker','date', ...].\")\n",
    "\n",
    "if \"X_panel\" not in globals() or not isinstance(X_panel, pd.DataFrame):\n",
    "    raise NameError(\"X_panel not found. Run your winsorize/zscore chunk first so X_panel exists with MultiIndex (date, ticker).\")\n",
    "\n",
    "px = px_stocks.copy()\n",
    "px[\"date\"] = pd.to_datetime(px[\"date\"], errors=\"coerce\")\n",
    "px[\"ticker\"] = px[\"ticker\"].astype(str)\n",
    "\n",
    "close_col = \"close\" if \"close\" in px.columns else (\"Close\" if \"Close\" in px.columns else None)\n",
    "if close_col is None:\n",
    "    raise KeyError(f\"Couldn't find a close column in px_stocks. Columns: {px.columns.tolist()}\")\n",
    "\n",
    "px = px.dropna(subset=[\"ticker\", \"date\", close_col]).sort_values([\"ticker\", \"date\"], kind=\"mergesort\")\n",
    "\n",
    "px_wide = px.pivot(index=\"date\", columns=\"ticker\", values=close_col).sort_index()\n",
    "px_wide = px_wide.ffill()\n",
    "\n",
    "h = 63  # ~3 months trading days\n",
    "fwd_ret = px_wide.shift(-h) / px_wide - 1.0\n",
    "\n",
    "y = fwd_ret.stack(dropna=False)\n",
    "y.index.names = [\"date\", \"ticker\"]\n",
    "y.name = \"ret_3m_fwd\"\n",
    "\n",
    "panel_index = X_panel.index\n",
    "y = y.reindex(panel_index)\n",
    "\n",
    "mask = y.notna()\n",
    "print(\"y coverage:\", float(mask.mean()), \"non-NA:\", int(mask.sum()), \"total:\", int(mask.size))\n",
    "print(\"y sample:\", y.dropna().iloc[:5].to_dict())\n",
    "\n",
    "y_ret3m = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9eef59e0-e677-4d91-b5bb-78abaae4dd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quarter-end decision dates: 58\n",
      "First / last q-end: 2005-03-31 00:00:00 2025-09-30 00:00:00\n",
      "\n",
      "Fold 1\n",
      "  Train: 2005-03-31 -> 2015-12-31 (32 q)\n",
      "  Val:   2016-03-31 -> 2019-12-31 (8 q)\n",
      "  Test:  2020-03-31 -> 2023-03-31 (12 q)\n",
      "\n",
      "Fold 2\n",
      "  Train: 2006-06-30 -> 2017-03-31 (32 q)\n",
      "  Val:   2017-06-30 -> 2020-12-31 (8 q)\n",
      "  Test:  2021-03-31 -> 2025-03-31 (12 q)\n",
      "\n",
      "Quarterly X shape: (539748, 50)\n",
      "Quarterly y coverage: 0.776664295189607\n",
      "\n",
      "Fold 1 rows\n",
      "  Train rows: 297792 y non-NA: 207451\n",
      "  Val rows:   74448 y non-NA: 62611\n",
      "  Test rows:  111672 y non-NA: 103553\n",
      "\n",
      "Fold 2 rows\n",
      "  Train rows: 297792 y non-NA: 216172\n",
      "  Val rows:   74448 y non-NA: 65491\n",
      "  Test rows:  111672 y non-NA: 106646\n"
     ]
    }
   ],
   "source": [
    "if \"X_panel\" not in globals() or not isinstance(X_panel, pd.DataFrame):\n",
    "    raise NameError(\"X_panel not found.\")\n",
    "if \"y_ret3m\" not in globals():\n",
    "    raise NameError(\"y_ret3m not found.\")\n",
    "\n",
    "dates = pd.to_datetime(X_panel.index.get_level_values(\"date\")).unique()\n",
    "dates = pd.DatetimeIndex(sorted(dates))\n",
    "\n",
    "q_end_dates = pd.DatetimeIndex(sorted(dates[dates.is_quarter_end]))\n",
    "\n",
    "print(\"Quarter-end decision dates:\", len(q_end_dates))\n",
    "print(\"First / last q-end:\", q_end_dates.min(), q_end_dates.max())\n",
    "\n",
    "train_q = 32\n",
    "val_q = 8\n",
    "test_q = 12\n",
    "step_q = 4\n",
    "\n",
    "folds = []\n",
    "i = 0\n",
    "while True:\n",
    "    train_start = i\n",
    "    train_end = train_start + train_q\n",
    "    val_end = train_end + val_q\n",
    "    test_end = val_end + test_q\n",
    "\n",
    "    if test_end > len(q_end_dates):\n",
    "        break\n",
    "\n",
    "    folds.append(\n",
    "        {\n",
    "            \"train\": q_end_dates[train_start:train_end],\n",
    "            \"val\": q_end_dates[train_end:val_end],\n",
    "            \"test\": q_end_dates[val_end:test_end],\n",
    "        }\n",
    "    )\n",
    "    i += step_q\n",
    "\n",
    "for k, f in enumerate(folds, 1):\n",
    "    print(f\"\\nFold {k}\")\n",
    "    print(\"  Train:\", f[\"train\"][0].date(), \"->\", f[\"train\"][-1].date(), f\"({len(f['train'])} q)\")\n",
    "    print(\"  Val:  \", f[\"val\"][0].date(), \"->\", f[\"val\"][-1].date(), f\"({len(f['val'])} q)\")\n",
    "    print(\"  Test: \", f[\"test\"][0].date(), \"->\", f[\"test\"][-1].date(), f\"({len(f['test'])} q)\")\n",
    "\n",
    "mask_q = X_panel.index.get_level_values(\"date\").isin(q_end_dates)\n",
    "Xq = X_panel.loc[mask_q].copy()\n",
    "yq = y_ret3m.reindex(Xq.index)\n",
    "\n",
    "print(\"\\nQuarterly X shape:\", Xq.shape)\n",
    "print(\"Quarterly y coverage:\", float(yq.notna().mean()))\n",
    "\n",
    "fold_datasets = []\n",
    "for k, f in enumerate(folds, 1):\n",
    "    d = Xq.index.get_level_values(\"date\")\n",
    "    train_mask = d.isin(f[\"train\"])\n",
    "    val_mask = d.isin(f[\"val\"])\n",
    "    test_mask = d.isin(f[\"test\"])\n",
    "\n",
    "    fold_datasets.append(\n",
    "        {\n",
    "            \"fold\": k,\n",
    "            \"train_dates\": f[\"train\"],\n",
    "            \"val_dates\": f[\"val\"],\n",
    "            \"test_dates\": f[\"test\"],\n",
    "            \"X_train\": Xq.loc[train_mask],\n",
    "            \"y_train\": yq.loc[train_mask],\n",
    "            \"X_val\": Xq.loc[val_mask],\n",
    "            \"y_val\": yq.loc[val_mask],\n",
    "            \"X_test\": Xq.loc[test_mask],\n",
    "            \"y_test\": yq.loc[test_mask],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"\\nFold {k} rows\")\n",
    "    print(\"  Train rows:\", int(train_mask.sum()), \"y non-NA:\", int(yq.loc[train_mask].notna().sum()))\n",
    "    print(\"  Val rows:  \", int(val_mask.sum()), \"y non-NA:\", int(yq.loc[val_mask].notna().sum()))\n",
    "    print(\"  Test rows: \", int(test_mask.sum()), \"y non-NA:\", int(yq.loc[test_mask].notna().sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30f58dea-9e51-4ab4-8c21-b375758bc1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quarterly OOS quarters:   0%|                                                                    | 0/24 [00:00<?, ?q/s]C:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [16:37:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "Quarterly OOS quarters:  29%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                         | 7/24 [00:06<00:11,  1.45q/s, fold=1 date=2023-03-31]C:\\Users\\Admin\\miniconda3\\Lib\\site-packages\\xgboost\\callback.py:386: UserWarning: [16:37:24] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"predictor\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "Quarterly OOS quarters: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [00:20<00:00,  1.17q/s, fold=2 date=2025-03-31]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perf rows: 24 fit folds: 2 weights rows: 2400 holdings rows: 2400\n",
      "\n",
      "Backtest stats (NET):\n",
      "n_periods               24.000000\n",
      "ann_return               0.411789\n",
      "ann_vol                  0.345616\n",
      "sharpe                   1.206711\n",
      "max_drawdown            -0.206253\n",
      "hit_rate                 0.666667\n",
      "avg_period_return        0.104265\n",
      "median_period_return     0.114979\n",
      "cum_return               6.918058\n",
      "dtype: float64\n",
      "\n",
      "Backtest stats (GROSS):\n",
      "n_periods               24.000000\n",
      "ann_return               0.414794\n",
      "ann_vol                  0.345611\n",
      "sharpe                   1.213350\n",
      "max_drawdown            -0.205653\n",
      "hit_rate                 0.666667\n",
      "avg_period_return        0.104837\n",
      "median_period_return     0.115564\n",
      "cum_return               7.019721\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(             fold  gross_ret   net_ret  turnover  n_universe  trained_rounds  \\\n",
       " signal_date                                                                    \n",
       " 2023-03-31      2  -0.052685 -0.053285      1.20        1785             423   \n",
       " 2023-06-30      2   0.103997  0.103367      1.26        1803             423   \n",
       " 2024-09-30      2   0.011251  0.010501      1.50        1757             423   \n",
       " 2024-12-31      2   0.102179  0.101629      1.10        1802             423   \n",
       " 2025-03-31      2  -0.205653 -0.206253      1.20        1759             423   \n",
       " \n",
       "              best_trees    val_ic  rmse_improve  \n",
       " signal_date                                      \n",
       " 2023-03-31          323 -0.008934      0.020526  \n",
       " 2023-06-30          323 -0.008934      0.020526  \n",
       " 2024-09-30          323 -0.008934      0.020526  \n",
       " 2024-12-31          323 -0.008934      0.020526  \n",
       " 2025-03-31          323 -0.008934      0.020526  ,\n",
       "    fold  trained_rounds  best_trees  val_rmse_best  val_rmse_last  rmse_model  \\\n",
       " 0     1             193          93       0.996348       0.997721    0.997721   \n",
       " 1     2             423         323       0.977786       0.979413    0.979413   \n",
       " \n",
       "    rmse_zero  rmse_improve    val_ic  train_rows  val_rows  \n",
       " 0   0.999936      0.002216  0.053385      207451     62611  \n",
       " 1   0.999939      0.020526 -0.008934      216172     65491  ,\n",
       "    fold signal_date ticker  side  weight  asset_return      pred\n",
       " 0     1  2020-03-31   ADSW  LONG    0.02     -0.080183  0.073570\n",
       " 1     1  2020-03-31   BLDR  LONG    0.02      0.692559  0.072361\n",
       " 2     1  2020-03-31    BRO  LONG    0.02      0.125345  0.050540\n",
       " 3     1  2020-03-31   CFFN  LONG    0.02     -0.051680  0.066021\n",
       " 4     1  2020-03-31   CLBK  LONG    0.02     -0.030903  0.046170,\n",
       "    fold signal_date ticker  weight\n",
       " 0     1  2020-03-31   ADSW    0.02\n",
       " 1     1  2020-03-31   AGRX   -0.02\n",
       " 2     1  2020-03-31  AHPIQ   -0.02\n",
       " 3     1  2020-03-31    AIM   -0.02\n",
       " 4     1  2020-03-31   APRN   -0.02)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#backtest\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "if \"fold_datasets\" not in globals() or len(fold_datasets) == 0:\n",
    "    raise NameError(\"fold_datasets not found. Run the combined folds+datasets cell first.\")\n",
    "\n",
    "if \"Xq\" not in globals() or \"yq\" not in globals():\n",
    "    raise NameError(\"Xq/yq not found. Run the combined folds+datasets cell first.\")\n",
    "\n",
    "SEED = int(globals().get(\"SEED\", 42))\n",
    "N_LONG = int(globals().get(\"N_LONG\", 50))\n",
    "N_SHORT = int(globals().get(\"N_SHORT\", 50))\n",
    "COST_BPS_ONE_WAY = float(globals().get(\"COST_BPS_ONE_WAY\", 0.0))\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "def _cs_zscore_series(y_s):\n",
    "    df = y_s.rename(\"y\").reset_index()\n",
    "    m = df.groupby(\"date\")[\"y\"].transform(\"mean\")\n",
    "    s = df.groupby(\"date\")[\"y\"].transform(\"std\").replace(0.0, np.nan)\n",
    "    out = (df[\"y\"] - m) / s\n",
    "    out.index = pd.MultiIndex.from_frame(df[[\"date\", \"ticker\"]])\n",
    "    out.index.names = [\"date\", \"ticker\"]\n",
    "    return out\n",
    "\n",
    "def _spearman_ic(pred_s, y_s):\n",
    "    df = pd.DataFrame({\"p\": pred_s, \"y\": y_s}).dropna()\n",
    "    if df.empty:\n",
    "        return np.nan\n",
    "    return df[\"p\"].rank().corr(df[\"y\"].rank())\n",
    "\n",
    "def _best_ntree(booster):\n",
    "    try:\n",
    "        a = booster.attributes()\n",
    "        if \"best_iteration\" in a:\n",
    "            return int(a[\"best_iteration\"]) + 1\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _stable_top_bottom(pred_s, n_long, n_short):\n",
    "    s = pred_s.dropna()\n",
    "    if s.empty:\n",
    "        return [], []\n",
    "    idx = s.index.astype(str).to_numpy()\n",
    "    val = s.to_numpy()\n",
    "    order_long = np.lexsort((idx, -val))\n",
    "    order_short = np.lexsort((idx, val))\n",
    "    longs = s.index.values[order_long][:n_long].tolist()\n",
    "    shorts = s.index.values[order_short][:n_short].tolist()\n",
    "    return longs, shorts\n",
    "\n",
    "base_params = {\n",
    "    \"max_depth\": 4,\n",
    "    \"eta\": 0.03,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"lambda\": 1.0,\n",
    "    \"alpha\": 0.0,\n",
    "    \"seed\": SEED,\n",
    "    \"nthread\": 1,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"predictor\": \"cpu_predictor\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "}\n",
    "\n",
    "feat_cols = list(Xq.columns)\n",
    "tickers_all = pd.Index(sorted(Xq.index.get_level_values(\"ticker\").astype(str).unique().tolist()))\n",
    "prev_w = pd.Series(0.0, index=tickers_all)\n",
    "\n",
    "rows_fit = []\n",
    "rows_perf = []\n",
    "rows_w = []\n",
    "rows_hold = []\n",
    "\n",
    "tasks = []\n",
    "for fd in fold_datasets:\n",
    "    test_dates = pd.Index(sorted(fd[\"X_test\"].index.get_level_values(\"date\").unique()))\n",
    "    for d in test_dates:\n",
    "        tasks.append((int(fd[\"fold\"]), pd.Timestamp(d)))\n",
    "\n",
    "pbar = tqdm(tasks, desc=\"Quarterly OOS quarters\", unit=\"q\", mininterval=0.5)\n",
    "\n",
    "fold_map = {int(fd[\"fold\"]): fd for fd in fold_datasets}\n",
    "trained_models = {}\n",
    "\n",
    "for fold_k, sig in pbar:\n",
    "    fd = fold_map[fold_k]\n",
    "\n",
    "    if fold_k not in trained_models:\n",
    "        X_train = fd[\"X_train\"].sort_index()\n",
    "        y_train = fd[\"y_train\"].reindex(X_train.index)\n",
    "\n",
    "        X_val = fd[\"X_val\"].sort_index()\n",
    "        y_val = fd[\"y_val\"].reindex(X_val.index)\n",
    "\n",
    "        y_train_cs = _cs_zscore_series(y_train)\n",
    "        y_val_cs = _cs_zscore_series(y_val)\n",
    "\n",
    "        mtr = y_train_cs.notna()\n",
    "        mva = y_val_cs.notna()\n",
    "\n",
    "        Xtr = X_train.loc[mtr, feat_cols].to_numpy(dtype=np.float32, copy=False)\n",
    "        ytr = y_train_cs.loc[mtr].to_numpy(dtype=np.float32, copy=False)\n",
    "        Xva = X_val.loc[mva, feat_cols].to_numpy(dtype=np.float32, copy=False)\n",
    "        yva = y_val_cs.loc[mva].to_numpy(dtype=np.float32, copy=False)\n",
    "\n",
    "        if len(ytr) == 0 or len(yva) == 0:\n",
    "            trained_models[fold_k] = None\n",
    "            continue\n",
    "\n",
    "        dtr = xgb.DMatrix(Xtr, label=ytr, feature_names=feat_cols)\n",
    "        dva = xgb.DMatrix(Xva, label=yva, feature_names=feat_cols)\n",
    "\n",
    "        evals_result = {}\n",
    "        booster = xgb.train(\n",
    "            params=base_params,\n",
    "            dtrain=dtr,\n",
    "            num_boost_round=5000,\n",
    "            evals=[(dva, \"val\")],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=False,\n",
    "            evals_result=evals_result,\n",
    "        )\n",
    "\n",
    "        nt = _best_ntree(booster)\n",
    "        try:\n",
    "            n_trained = int(booster.num_boosted_rounds())\n",
    "        except Exception:\n",
    "            n_trained = np.nan\n",
    "\n",
    "        val_rmse_path = evals_result.get(\"val\", {}).get(\"rmse\", [])\n",
    "        val_rmse_best = float(np.min(val_rmse_path)) if len(val_rmse_path) else np.nan\n",
    "        val_rmse_last = float(val_rmse_path[-1]) if len(val_rmse_path) else np.nan\n",
    "\n",
    "        val_pred = booster.predict(dva)\n",
    "        rmse_model = float(np.sqrt(np.mean((val_pred - yva) ** 2)))\n",
    "        rmse_zero = float(np.sqrt(np.mean((0.0 - yva) ** 2)))\n",
    "        rmse_improve = rmse_zero - rmse_model\n",
    "        ic = _spearman_ic(pd.Series(val_pred), pd.Series(yva))\n",
    "\n",
    "        rows_fit.append(\n",
    "            {\n",
    "                \"fold\": fold_k,\n",
    "                \"trained_rounds\": n_trained,\n",
    "                \"best_trees\": int(nt) if nt is not None else np.nan,\n",
    "                \"val_rmse_best\": val_rmse_best,\n",
    "                \"val_rmse_last\": val_rmse_last,\n",
    "                \"rmse_model\": rmse_model,\n",
    "                \"rmse_zero\": rmse_zero,\n",
    "                \"rmse_improve\": rmse_improve,\n",
    "                \"val_ic\": float(ic) if ic == ic else np.nan,\n",
    "                \"train_rows\": int(len(ytr)),\n",
    "                \"val_rows\": int(len(yva)),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        trained_models[fold_k] = {\n",
    "            \"booster\": booster,\n",
    "            \"nt\": nt,\n",
    "            \"n_trained\": n_trained,\n",
    "            \"val_ic\": float(ic) if ic == ic else np.nan,\n",
    "            \"rmse_improve\": rmse_improve,\n",
    "        }\n",
    "\n",
    "    model_pack = trained_models.get(fold_k)\n",
    "    if model_pack is None:\n",
    "        continue\n",
    "\n",
    "    booster = model_pack[\"booster\"]\n",
    "    nt = model_pack[\"nt\"]\n",
    "    n_trained = model_pack[\"n_trained\"]\n",
    "    ic = model_pack[\"val_ic\"]\n",
    "    rmse_improve = model_pack[\"rmse_improve\"]\n",
    "\n",
    "    X_test = fd[\"X_test\"].sort_index()\n",
    "    y_test = fd[\"y_test\"].reindex(X_test.index)\n",
    "\n",
    "    try:\n",
    "        Xsig = X_test.loc[(sig, slice(None)), feat_cols]\n",
    "        ysig = y_test.loc[(sig, slice(None))]\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "    if Xsig.empty:\n",
    "        continue\n",
    "\n",
    "    Xsig = Xsig.sort_index()\n",
    "    Xsig_np = Xsig.to_numpy(dtype=np.float32, copy=False)\n",
    "    pred = pd.Series(\n",
    "        booster.predict(xgb.DMatrix(Xsig_np, feature_names=feat_cols)),\n",
    "        index=Xsig.index.get_level_values(\"ticker\").astype(str),\n",
    "    )\n",
    "\n",
    "    ysig = ysig.copy()\n",
    "    ysig.index = ysig.index.get_level_values(\"ticker\").astype(str)\n",
    "\n",
    "    universe0 = pred.index.intersection(ysig.index)\n",
    "\n",
    "    MIN_PRICE = float(globals().get(\"MIN_PRICE\", 5.0))\n",
    "    MIN_DVOL_20 = float(globals().get(\"MIN_DVOL_20\", 1e6))\n",
    "    \n",
    "    core = Xsig.copy()\n",
    "    core.index = core.index.get_level_values(\"ticker\").astype(str)\n",
    "    \n",
    "    px_ok = core[\"px\"] >= MIN_PRICE if \"px\" in core.columns else pd.Series(True, index=core.index)\n",
    "    liq_ok = core[\"dvol_20\"] >= MIN_DVOL_20 if \"dvol_20\" in core.columns else pd.Series(True, index=core.index)\n",
    "    \n",
    "    universe = [t for t in universe0 if bool(px_ok.get(t, False)) and bool(liq_ok.get(t, False))]\n",
    "    \n",
    "    pred_u = pred.loc[universe].dropna()\n",
    "    y_u = ysig.loc[universe]\n",
    "    \n",
    "    if len(pred_u) < (N_LONG + N_SHORT):\n",
    "        continue\n",
    "    \n",
    "    longs, shorts = _stable_top_bottom(pred_u, N_LONG, N_SHORT)\n",
    "    if len(longs) == 0 or len(shorts) == 0:\n",
    "        continue\n",
    "\n",
    "    w = pd.Series(0.0, index=tickers_all)\n",
    "    w.loc[longs] = 1.0 / len(longs)\n",
    "    w.loc[shorts] = -1.0 / len(shorts)\n",
    "\n",
    "    held = longs + shorts\n",
    "    y_held = y_u.reindex(held).astype(\"float64\").dropna()\n",
    "    if y_held.empty:\n",
    "        continue\n",
    "\n",
    "    w_held = w.reindex(y_held.index).astype(\"float64\")\n",
    "    gross = float((w_held * y_held).sum())\n",
    "\n",
    "    turnover = 0.5 * float((w - prev_w).abs().sum())\n",
    "    cost = turnover * (COST_BPS_ONE_WAY / 1e4)\n",
    "    net = gross - cost\n",
    "\n",
    "    rows_perf.append(\n",
    "        {\n",
    "            \"fold\": fold_k,\n",
    "            \"signal_date\": pd.Timestamp(sig),\n",
    "            \"gross_ret\": gross,\n",
    "            \"net_ret\": net,\n",
    "            \"turnover\": turnover,\n",
    "            \"n_universe\": int(len(pred_u)),\n",
    "            \"trained_rounds\": n_trained,\n",
    "            \"best_trees\": int(nt) if nt is not None else np.nan,\n",
    "            \"val_ic\": float(ic) if ic == ic else np.nan,\n",
    "            \"rmse_improve\": rmse_improve,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    w_sig = w[w != 0.0]\n",
    "    for tk, ww in w_sig.items():\n",
    "        rows_w.append({\"fold\": fold_k, \"signal_date\": pd.Timestamp(sig), \"ticker\": str(tk), \"weight\": float(ww)})\n",
    "\n",
    "    for tk in y_held.index:\n",
    "        side = \"LONG\" if w.loc[tk] > 0 else \"SHORT\"\n",
    "        rows_hold.append(\n",
    "            {\n",
    "                \"fold\": fold_k,\n",
    "                \"signal_date\": pd.Timestamp(sig),\n",
    "                \"ticker\": str(tk),\n",
    "                \"side\": side,\n",
    "                \"weight\": float(w.loc[tk]),\n",
    "                \"asset_return\": float(y_held.loc[tk]),\n",
    "                \"pred\": float(pred_u.loc[tk]) if tk in pred_u.index else np.nan,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    prev_w = w\n",
    "    pbar.set_postfix_str(f\"fold={fold_k} date={sig.date()}\")\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "perf = pd.DataFrame(rows_perf).sort_values([\"signal_date\", \"fold\"]).set_index(\"signal_date\")\n",
    "fit_diag = pd.DataFrame(rows_fit).sort_values([\"fold\"]).reset_index(drop=True)\n",
    "weights = pd.DataFrame(rows_w).sort_values([\"signal_date\", \"fold\", \"ticker\"]).reset_index(drop=True)\n",
    "holdings = pd.DataFrame(rows_hold).sort_values([\"signal_date\", \"fold\", \"side\", \"ticker\"]).reset_index(drop=True)\n",
    "\n",
    "def _perf_stats(r):\n",
    "    r = pd.Series(r).dropna()\n",
    "    if r.empty:\n",
    "        return pd.Series(dtype=\"float64\")\n",
    "    ann_factor = 4.0\n",
    "    n = float(len(r))\n",
    "    equity = (1.0 + r).cumprod()\n",
    "    ann_ret = equity.iloc[-1] ** (ann_factor / n) - 1.0\n",
    "    ann_vol = r.std(ddof=0) * np.sqrt(ann_factor)\n",
    "    sharpe = (r.mean() / r.std(ddof=0) * np.sqrt(ann_factor)) if r.std(ddof=0) != 0 else np.nan\n",
    "    running_max = equity.cummax()\n",
    "    dd = equity / running_max - 1.0\n",
    "    mdd = dd.min()\n",
    "    hit = (r > 0).mean()\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"n_periods\": n,\n",
    "            \"ann_return\": float(ann_ret),\n",
    "            \"ann_vol\": float(ann_vol),\n",
    "            \"sharpe\": float(sharpe) if sharpe == sharpe else np.nan,\n",
    "            \"max_drawdown\": float(mdd),\n",
    "            \"hit_rate\": float(hit),\n",
    "            \"avg_period_return\": float(r.mean()),\n",
    "            \"median_period_return\": float(r.median()),\n",
    "            \"cum_return\": float(equity.iloc[-1] - 1.0),\n",
    "        }\n",
    "    )\n",
    "\n",
    "stats_net = _perf_stats(perf[\"net_ret\"]) if \"net_ret\" in perf.columns else pd.Series(dtype=\"float64\")\n",
    "stats_gross = _perf_stats(perf[\"gross_ret\"]) if \"gross_ret\" in perf.columns else pd.Series(dtype=\"float64\")\n",
    "\n",
    "print(\"perf rows:\", len(perf), \"fit folds:\", len(fit_diag), \"weights rows:\", len(weights), \"holdings rows:\", len(holdings))\n",
    "print(\"\\nBacktest stats (NET):\")\n",
    "print(stats_net)\n",
    "print(\"\\nBacktest stats (GROSS):\")\n",
    "print(stats_gross)\n",
    "\n",
    "perf.tail(), fit_diag, holdings.head(), weights.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19cd0a42-9a29-4bcb-a13a-63d6d4c26ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean IC: 0.23768586119893934\n",
      "IC std: 0.27136433177548264\n",
      "IC t-stat: 4.290977190717072\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal_date</th>\n",
       "      <th>fold</th>\n",
       "      <th>ic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.082616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>2</td>\n",
       "      <td>0.175976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.018758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>2</td>\n",
       "      <td>0.246853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2025-03-31</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.256166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   signal_date  fold        ic\n",
       "19  2023-03-31     2 -0.082616\n",
       "20  2023-06-30     2  0.175976\n",
       "21  2024-09-30     2 -0.018758\n",
       "22  2024-12-31     2  0.246853\n",
       "23  2025-03-31     2 -0.256166"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IC time series\n",
    "\n",
    "if \"holdings\" not in globals():\n",
    "    raise NameError(\"holdings not found.\")\n",
    "\n",
    "ic_rows = []\n",
    "for (d, f), grp in holdings.groupby([\"signal_date\",\"fold\"]):\n",
    "    if grp.shape[0] < 50:\n",
    "        continue\n",
    "    ic = grp[\"pred\"].rank().corr(grp[\"asset_return\"].rank())\n",
    "    ic_rows.append({\"signal_date\": d, \"fold\": f, \"ic\": ic})\n",
    "\n",
    "ic_df = pd.DataFrame(ic_rows).sort_values(\"signal_date\")\n",
    "print(\"Mean IC:\", ic_df[\"ic\"].mean())\n",
    "print(\"IC std:\", ic_df[\"ic\"].std())\n",
    "print(\"IC t-stat:\", ic_df[\"ic\"].mean() / (ic_df[\"ic\"].std() / np.sqrt(len(ic_df))))\n",
    "\n",
    "ic_df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92f83ea6-1a6e-4f9f-a0c5-f80024a502db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               signal_date       fold  ls_spread\n",
      "count                   24  24.000000  24.000000\n",
      "mean   2022-04-23 05:00:00   1.500000   0.108251\n",
      "min    2020-03-31 00:00:00   1.000000  -0.157342\n",
      "25%    2021-06-07 06:00:00   1.000000  -0.049100\n",
      "50%    2022-02-14 00:00:00   1.500000   0.067542\n",
      "75%    2022-11-14 12:00:00   2.000000   0.208911\n",
      "max    2025-03-31 00:00:00   2.000000   0.529449\n",
      "std                    NaN   0.510754   0.191699\n"
     ]
    }
   ],
   "source": [
    "#decile test\n",
    "\n",
    "decile_rows = []\n",
    "\n",
    "for (d, f), grp in holdings.groupby([\"signal_date\",\"fold\"]):\n",
    "    grp = grp.dropna(subset=[\"pred\",\"asset_return\"])\n",
    "    if len(grp) < 100:\n",
    "        continue\n",
    "\n",
    "    grp[\"decile\"] = pd.qcut(grp[\"pred\"], 10, labels=False, duplicates=\"drop\")\n",
    "    dec = grp.groupby(\"decile\")[\"asset_return\"].mean()\n",
    "\n",
    "    for k, v in dec.items():\n",
    "        decile_rows.append({\"signal_date\": d, \"fold\": f, \"decile\": int(k), \"ret\": v})\n",
    "\n",
    "dec_df = pd.DataFrame(decile_rows)\n",
    "dec_summary = dec_df.groupby(\"decile\")[\"ret\"].mean()\n",
    "print(dec_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31ad1c3d-cf65-4576-91b3-87e5dd71117d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               signal_date       fold  ls_spread\n",
      "count                   24  24.000000  24.000000\n",
      "mean   2022-04-23 05:00:00   1.500000   0.108251\n",
      "min    2020-03-31 00:00:00   1.000000  -0.157342\n",
      "25%    2021-06-07 06:00:00   1.000000  -0.049100\n",
      "50%    2022-02-14 00:00:00   1.500000   0.067542\n",
      "75%    2022-11-14 12:00:00   2.000000   0.208911\n",
      "max    2025-03-31 00:00:00   2.000000   0.529449\n",
      "std                    NaN   0.510754   0.191699\n"
     ]
    }
   ],
   "source": [
    "# long short spread\n",
    "ls_rows = []\n",
    "\n",
    "for (d, f), grp in holdings.groupby([\"signal_date\",\"fold\"]):\n",
    "    grp = grp.dropna(subset=[\"pred\",\"asset_return\"])\n",
    "    if len(grp) < 100:\n",
    "        continue\n",
    "\n",
    "    grp[\"decile\"] = pd.qcut(grp[\"pred\"], 10, labels=False, duplicates=\"drop\")\n",
    "    top = grp[grp[\"decile\"] == grp[\"decile\"].max()][\"asset_return\"].mean()\n",
    "    bot = grp[grp[\"decile\"] == grp[\"decile\"].min()][\"asset_return\"].mean()\n",
    "\n",
    "    ls_rows.append({\n",
    "        \"signal_date\": d,\n",
    "        \"fold\": f,\n",
    "        \"ls_spread\": top - bot\n",
    "    })\n",
    "\n",
    "ls_df = pd.DataFrame(ls_rows)\n",
    "print(ls_df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83e075ca-d59d-4d5b-9767-68a9508e784a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-2015:\n",
      "nan\n",
      "2015\u20132019:\n",
      "nan\n",
      "2020+ :\n",
      "0.1042647003186491\n"
     ]
    }
   ],
   "source": [
    "perf_reset = perf.reset_index()\n",
    "perf_reset[\"year\"] = perf_reset[\"signal_date\"].dt.year\n",
    "\n",
    "print(\"Pre-2015:\")\n",
    "print(perf_reset[perf_reset[\"year\"] < 2015][\"net_ret\"].mean())\n",
    "\n",
    "print(\"2015\u20132019:\")\n",
    "print(perf_reset[(perf_reset[\"year\"] >= 2015) & (perf_reset[\"year\"] < 2020)][\"net_ret\"].mean())\n",
    "\n",
    "print(\"2020+ :\")\n",
    "print(perf_reset[perf_reset[\"year\"] >= 2020][\"net_ret\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61410449-f29c-4554-b353-439bbbe7dc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature\n",
       "assetturnover               436.531479\n",
       "val_ps                      362.882805\n",
       "ebitdamargin                318.630547\n",
       "val_evebitda                305.299355\n",
       "dvol_20                     288.611992\n",
       "capex_assets                272.133812\n",
       "asset_growth_yoy            268.998268\n",
       "lev_debt_assets__missing    254.822060\n",
       "ret_63                      248.054756\n",
       "val_evebitda__missing       246.002060\n",
       "eps_yoy                     244.078262\n",
       "val_pb                      227.288643\n",
       "rev_yoy                     220.118164\n",
       "wc_chg_assets               217.192574\n",
       "ret_21                      215.994720\n",
       "Name: gain, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = []\n",
    "\n",
    "for fd in fold_datasets:\n",
    "    k = fd[\"fold\"]\n",
    "    model = trained_models.get(k, {}).get(\"booster\")\n",
    "    if model is None:\n",
    "        continue\n",
    "    imp = model.get_score(importance_type=\"gain\")\n",
    "    for f, v in imp.items():\n",
    "        importances.append({\"fold\": k, \"feature\": f, \"gain\": v})\n",
    "\n",
    "imp_df = pd.DataFrame(importances)\n",
    "imp_avg = imp_df.groupby(\"feature\")[\"gain\"].mean().sort_values(ascending=False)\n",
    "imp_avg.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a898592-232f-4df9-98f0-b69b70fabc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stats and export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "if \"perf\" not in globals() or perf.empty:\n",
    "    raise NameError(\"perf not found or empty.\")\n",
    "if \"weights\" not in globals() or weights.empty:\n",
    "    raise NameError(\"weights not found or empty.\")\n",
    "if \"holdings\" not in globals() or holdings.empty:\n",
    "    raise NameError(\"holdings not found or empty.\")\n",
    "\n",
    "perf_reset = perf.reset_index().copy()\n",
    "perf_reset[\"signal_date\"] = pd.to_datetime(perf_reset[\"signal_date\"])\n",
    "perf_reset[\"fold\"] = perf_reset[\"fold\"].astype(int)\n",
    "\n",
    "perf_live = perf_reset.sort_values([\"signal_date\",\"fold\"]).drop_duplicates(subset=[\"signal_date\"], keep=\"last\").set_index(\"signal_date\").sort_index()\n",
    "\n",
    "weights_live = weights.copy()\n",
    "weights_live[\"signal_date\"] = pd.to_datetime(weights_live[\"signal_date\"])\n",
    "weights_live[\"fold\"] = weights_live[\"fold\"].astype(int)\n",
    "weights_live = weights_live.sort_values([\"signal_date\",\"fold\",\"ticker\"]).drop_duplicates(subset=[\"signal_date\",\"ticker\"], keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "holdings_live = holdings.copy()\n",
    "holdings_live[\"signal_date\"] = pd.to_datetime(holdings_live[\"signal_date\"])\n",
    "holdings_live[\"fold\"] = holdings_live[\"fold\"].astype(int)\n",
    "holdings_live = holdings_live.sort_values([\"signal_date\",\"fold\",\"ticker\"]).drop_duplicates(subset=[\"signal_date\",\"ticker\"], keep=\"last\").reset_index(drop=True)\n",
    "\n",
    "def _perf_stats(r):\n",
    "    r = pd.Series(r).dropna()\n",
    "    if r.empty:\n",
    "        return pd.Series(dtype=\"float64\")\n",
    "    ann_factor = 4.0\n",
    "    n = float(len(r))\n",
    "    equity = (1.0 + r).cumprod()\n",
    "    ann_ret = equity.iloc[-1] ** (ann_factor / n) - 1.0\n",
    "    ann_vol = r.std(ddof=0) * np.sqrt(ann_factor)\n",
    "    sharpe = (r.mean() / r.std(ddof=0) * np.sqrt(ann_factor)) if r.std(ddof=0) != 0 else np.nan\n",
    "    running_max = equity.cummax()\n",
    "    dd = equity / running_max - 1.0\n",
    "    mdd = dd.min()\n",
    "    hit = (r > 0).mean()\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"n_quarters\": n,\n",
    "            \"ann_return\": float(ann_ret),\n",
    "            \"ann_vol\": float(ann_vol),\n",
    "            \"sharpe\": float(sharpe) if sharpe == sharpe else np.nan,\n",
    "            \"max_drawdown\": float(mdd),\n",
    "            \"hit_rate\": float(hit),\n",
    "            \"avg_q_return\": float(r.mean()),\n",
    "            \"median_q_return\": float(r.median()),\n",
    "            \"cum_return\": float(equity.iloc[-1] - 1.0),\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"perf rows (all folds):\", len(perf_reset))\n",
    "print(\"perf rows (live dedup):\", len(perf_live))\n",
    "\n",
    "print(\"\\nLIVE Backtest stats (NET):\")\n",
    "print(_perf_stats(perf_live[\"net_ret\"]))\n",
    "\n",
    "print(\"\\nLIVE Backtest stats (GROSS):\")\n",
    "print(_perf_stats(perf_live[\"gross_ret\"]))\n",
    "\n",
    "export_date = perf_live.index.max()\n",
    "print(\"\\nExporting holdings for:\", export_date.date())\n",
    "\n",
    "wq = weights_live[weights_live[\"signal_date\"] == export_date].copy()\n",
    "hq = holdings_live[holdings_live[\"signal_date\"] == export_date].copy()\n",
    "\n",
    "longs = hq[hq[\"side\"] == \"LONG\"][[\"ticker\",\"weight\",\"asset_return\"]].sort_values(\"weight\", ascending=False).head(50).reset_index(drop=True)\n",
    "shorts = hq[hq[\"side\"] == \"SHORT\"][[\"ticker\",\"weight\",\"asset_return\"]].sort_values(\"weight\", ascending=True).head(50).reset_index(drop=True)\n",
    "\n",
    "out_path = Path(CACHE_DIR) / f\"quarterly_holdings_{export_date.date()}_LIVE.xlsx\"\n",
    "with pd.ExcelWriter(out_path, engine=\"openpyxl\") as writer:\n",
    "    longs.to_excel(writer, sheet_name=\"LONG\", index=False)\n",
    "    shorts.to_excel(writer, sheet_name=\"SHORT\", index=False)\n",
    "\n",
    "print(\"Saved:\", out_path)\n",
    "\n",
    "perf_live.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d2814-df49-461f-95e1-428b35fd6600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c17de00-76a7-4e80-898b-f152f22d676e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9434b3-15cb-42ee-9011-b81fcbff42f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60827edd-65c1-476b-952c-9079eb1e98aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}